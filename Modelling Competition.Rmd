---
title: "Modelling Competition: House Prices"
author: 
- Roberto Alvarez Llordachs
- Javier Goñi Artieda
- Marcos Crespo Díaz
- Gabriela Levenfeld Sabau
- Sofía Gianelli Nan
- Miguel Díaz-Plaza Cabrera
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    toc_depth: 4
    number_sections: yes
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE, cache=TRUE)
```

# Introduction

House prices have sparked lot of debate recently in our society. Currently, this topic has aroused controversy on the grounds that has been a cornerstone in 2008 crisis. Were buyers to decide a model for setting the selling price, which factors would be included and what would be their weight?

The goal of this project is to create a effective price prediction model.

# Libraries

```{r, message=FALSE, warning=FALSE}
library(skimr)
library(dplyr)
library(tidyverse)
library(caret)
library(plotly)
library(data.table)
library(GGally)
library(tidymodels)
library(car)
library(scales)
library(MASS)
library(lmtest)
library(base)
library(corrplot)
```

# Data Preparation

First step would be to load both databases, test and test.

```{r}
train <- read.csv("train.csv", sep=",")
test <- read.csv("test.csv", sep=",")
```

We are going to work with the target variable (SalePrice) in log

```{r}
train$SalePrice<-log(train$SalePrice)
```

## We make EDA (Exploratory Data Analysis) of the test set

We are only going to work with the test set due to the test set is for studying the accuracy of the model.

```{r}
skim(train) 
skim(test)
#Presents results for every column; the statistics it provides depend on the class of the variable
```


```{r}
# Ordenar las columnas por tipo de variable (Las 25 primeras son continuas)
train <- train[,c(1,81,4,5,20,21,27,35,37,38,39,44,45,46,47,60,63,67,68,69,70,71,72,76,78,48,49,50,51,52,53,55,57,62,77,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,22,23,24,25,26,28,29,30,31,32,33,34,36,40,41,42,43,54,56,58,59,61,64,65,66,73,74,75,79,80)]

test <- test[,c(1,4,5,20,21,27,35,37,38,39,44,45,46,47,60,63,67,68,69,70,71,72,76,78,48,49,50,51,52,53,55,57,62,77,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,22,23,24,25,26,28,29,30,31,32,33,34,36,40,41,42,43,54,56,58,59,61,64,65,66,73,74,75,79,80)]
```

ID columns is irrelevant so we can delete it. The number of the observation is given by the row of each dataset.

```{r}
# Eliminate the ID columns
train <- train[,-1]
test <- test[,-1]
```

## Missing value problem:

Looking at the dataset, we observe that some of the variables have great deal of NA values. Therefore, it may interesting to quantify these numbers and later think on future actions.

```{r}
# Counting NA: train and express in %
Number_NA_train<-(colSums(is.na(train))/nrow(train))*100
sort(Number_NA_train,decreasing = TRUE)
```

```{r}
# Counting NA: test and express in %
Number_NA_test<-(colSums(is.na(test))/nrow(test))*100
sort(Number_NA_test,decreasing = TRUE)
```

Acoording to the high percentage of NA, we are going to eliminate the variables "PoolQC" and the associated numerical one, "PoolArea". The same occur with "MiscFeature" and "MiscVal", "Fence" and "Alley". 

Other variables are going to be deleted due to the great deal of values in a single category, which would imply that the rest of the categories are seen as outliers. This is the case of: Utilities, Street, RoofMatl, Condition2, Condition1, CentralAir, Heating, LandSlope and PavedDrive.

Furthermore, we delete some variables as we consider to have the similar meaning, those that have Qual and Cond:

  - BsmtQual and BsmtCond, deleting BsmtCond.
  
  - GarageQual and GarageCond, deleting GarageQual.

Finally, some numerical variables have for most of their observations (>90%) the same value, so for the same reason are going to be deleted. This is LowQualFinSF. 


```{r}
train <- train[,-c(13,22,23,37,38,73,75,76,77,78,41,45,46,52,64,66,43,60)]
test <- test[,-c(12,21,22,36,37,72,74,75,76,77,40,44,45,51,63,65,42,59)]
```

What should we then do this those variables that have more missing values than existing ones? Maintaining them can we a problem as makes no sense to substitute missing values with an statistic of the remaining, such as the mean, as the latter would not be representative. 
However, NA values have a meaning in most of the cases where its percentage is high. For example, in PoolQC it means that the house does not have pool, in Alley means that the house no alley access... Consequently, we should change NA for None so that they will later appear when displaying these variables values.

```{r}
train$BsmtFinType1 <- ifelse(is.na(train$BsmtFinType1),"None",train$BsmtFinType1)
train$BsmtQual <- ifelse(is.na(train$BsmtQual),"None",train$BsmtQual)
train$BsmtFinType2 <- ifelse(is.na(train$BsmtFinType2),"None",train$BsmtFinType2)
train$BsmtExposure <- ifelse(is.na(train$BsmtExposure),"None",train$BsmtExposure)
train$FireplaceQu <- ifelse(is.na(train$FireplaceQu),"None",train$FireplaceQu)
train$GarageCond <- ifelse(is.na(train$GarageCond),"None",train$GarageCond)
train$GarageFinish <- ifelse(is.na(train$GarageFinish),"None",train$GarageFinish)
train$GarageType <- ifelse(is.na(train$GarageType),"None",train$GarageType)
train$MasVnrType <- ifelse(is.na(train$MasVnrType),"None",train$MasVnrType)

test$BsmtFinType1 <- ifelse(is.na(test$BsmtFinType1),"None",test$BsmtFinType1)
test$BsmtQual <- ifelse(is.na(test$BsmtQual),"None",test$BsmtQual)
test$BsmtFinType2 <- ifelse(is.na(test$BsmtFinType2),"None",test$BsmtFinType2)
test$BsmtExposure <- ifelse(is.na(test$BsmtExposure),"None",test$BsmtExposure)
test$FireplaceQu <- ifelse(is.na(test$FireplaceQu),"None",test$FireplaceQu)
test$GarageCond <- ifelse(is.na(test$GarageCond),"None",test$GarageCond)
test$GarageFinish <- ifelse(is.na(test$GarageFinish),"None",test$GarageFinish)
test$GarageType <- ifelse(is.na(test$GarageType),"None",test$GarageType)
test$MasVnrType <- ifelse(is.na(test$MasVnrType),"None",test$MasVnrType)
```

Checking Garage variables we suspect that the NA correspond to not having a garage. When the variable is categorical we change it for None, but the problem comes when it is numerical, as imputing 0 for the missing values makes no sense. Numerical variables associated with garage, such as GarageArea or GarageCars have a value of 0 when there is no garage, which as mentioned, makes no sense. Therefore, we will create a dummy variable for first checking if the house has or not garage, so having missing values in GarageYrBuild would not cause a problem. The creation of a dummy variable, allow us to transform the 0 values of GarageArea and GarageCars to NA´s, so as not to affect the mean, median... 

Same process would be applied to other variables, such as MasVnrArea, MasVnrType.

```{r garage}
train$GarageArea<- ifelse(train$GarageArea==0,NA,train$GarageArea)
train$GarageCars<- ifelse(train$GarageCars==0,NA,train$GarageCars)
train$MasVnrArea<-ifelse(train$MasVnrArea==0,NA,train$MasVnrArea)
train$GarageYrBlt<-ifelse(train$GarageYrBlt==0,NA,train$GarageYrBlt)

test$GarageArea<- ifelse(test$GarageArea==0,NA,test$GarageArea)
test$GarageCars<- ifelse(test$GarageCars==0,NA,test$GarageCars)
test$MasVnrArea<-ifelse(test$MasVnrArea==0,NA,test$MasVnrArea)
test$GarageYrBlt<-ifelse(test$GarageYrBlt==0,NA,test$GarageYrBlt)
```

We will fix the missing values in the test data, changing for the mode in categorical variables and for the median in numerical ones.

```{r}
train$Electrical <- ifelse(is.na(train$Electrical),"SBrkr",train$Electrical) 

test$SaleType <- ifelse(is.na(test$SaleType),"WD",test$SaleType)
test$KitchenQual <- ifelse(is.na(test$KitchenQual),"TA",test$KitchenQual)
test$Exterior2nd <- ifelse(is.na(test$Exterior2nd),"VinylSd",test$Exterior2nd)
test$Exterior1st <- ifelse(is.na(test$Exterior1st),"VinylSd",test$Exterior1st)
test$TotalBsmtSF<- ifelse(is.na(test$TotalBsmtSF),median(na.omit(test$TotalBsmtSF)),test$TotalBsmtSF) #numeric
test$BsmtUnfSF<- ifelse(is.na(test$BsmtUnfSF),median(na.omit(test$BsmtUnfSF)),test$BsmtUnfSF) #numeric
test$BsmtFinSF2<- ifelse(is.na(test$BsmtFinSF2),median(na.omit(test$BsmtFinSF2)),test$BsmtFinSF2) #numeric
test$BsmtFinSF1<- ifelse(is.na(test$BsmtFinSF1),median(na.omit(test$BsmtFinSF1)),test$BsmtFinSF1) #numeric
test$BsmtFullBath<- ifelse(is.na(test$BsmtFullBath),median(na.omit(test$BsmtFullBath)),test$BsmtFullBath) #discrete
test$BsmtHalfBath<- ifelse(is.na(test$BsmtHalfBath),median(na.omit(test$BsmtHalfBath)),test$BsmtHalfBath) #discrete
test$Functional<- ifelse(is.na(test$Functional),"Typ",test$Functional)
test$MSZoning<- ifelse(is.na(test$MSZoning),"RL",test$MSZoning)
```

It is worth to mention the case of LotFrontage, as the amount of missing values is high, special attention should be paid. Instead of computing just the median of the data, we have used a multivariate imputation method, so that we have make the median based on other attribute that is related to this variable, such as the neighborhood, with the idea that in the same neighborhood the LotFrontage would be similar.
 
```{r frontage}
# Calcular la mediana por grupo utilizando tapply
medianas_por_grupo_train <- tapply(train$LotFrontage, train$Neighborhood, FUN = function(x) median(x, na.rm = TRUE))
medianas_por_grupo_test <- tapply(test$LotFrontage, test$Neighborhood, FUN = function(x) median(x, na.rm = TRUE))

# Sustituir NA por la mediana correspondiente por grupo
train$LotFrontage <- ifelse(is.na(train$LotFrontage), medianas_por_grupo_train[match(train$Neighborhood,names(medianas_por_grupo_train))],
                                   train$LotFrontage)

test$LotFrontage <- ifelse(is.na(test$LotFrontage), medianas_por_grupo_test[match(test$Neighborhood,names(medianas_por_grupo_test))],
                                   test$LotFrontage)

```

```{r}
#Inicialmente tenemos 259 missing values.
sum(is.na(train$LotFrontage))
sum(is.na(test$LotFrontage))
# Ahora ya tenemos 0 missing values
```

Additionally we see that YearRemodAdd, has construction date if no remodeling or additions have been made. For us that makes no sense, we will create a dummy variable if the house have been remodel and include NA´s when no remodel has been made, so as not to affect the mean, minimum and maximum.

```{r}
train$YearRemodAdd <- ifelse(train$YearRemodAdd==train$YearBuilt,NA,train$YearRemodAdd)
test$YearRemodAdd <- ifelse(test$YearRemodAdd==test$YearBuilt,NA,test$YearRemodAdd)
```

```{r}
#Me sale NA asumo que ya esta bien
sum(train$YearRemodAdd==train$YearBuilt)
sum(test$YearRemodAdd==test$YearBuilt)
```

```{r}
train$dummy_garage <- ifelse(is.na(train$GarageYrBlt),0,1)
train$dummy_masvnr <- ifelse(is.na(train$MasVnrArea),0,1)
train$dummy_RemodYear <- ifelse(is.na(train$YearRemodAdd),0,1)

test$dummy_garage <- ifelse(is.na(test$GarageYrBlt),0,1)
test$dummy_masvnr <- ifelse(is.na(test$MasVnrArea),0,1)
test$dummy_RemodYear <- ifelse(is.na(test$YearRemodAdd),0,1)
#Keep dummies as numeric
class(test$dummy_RemodYear)
```

Other important action in data preprocessing is to check if there are duplicated categories referring to the same thing. That was done manually and it seems that no duplicated categories exist.

Moreover, we should check if there are duplicated observations, using the duplicated functions.

```{r duplicates}
sum(duplicated(train))
sum(duplicated(test))
```

## Handle categorical variables:

Next, the goal is to modify categorical variables as a factor so as to work with them.

```{r}
train <- train %>% mutate_if(~is.character(.), ~as.factor(.))
#Nonetheless not all categorical variables are character defined, we modify using factor those who aren´t
train$OverallQual <- factor(train$OverallQual)
train$OverallCond <- factor(train$OverallCond)
train$MSSubClass <- factor(train$MSSubClass)

test <- test %>% mutate_if(~is.character(.), ~as.factor(.))
#Nonetheless not all categorical variables are character defined, we modify using factor those who aren´t
test$OverallQual <- factor(test$OverallQual)
test$OverallCond <- factor(test$OverallCond)
test$MSSubClass <- factor(test$MSSubClass)
```

**CON OVERALL's NO SABEMOS AUN SI TRATAR COMO NUMERICA O COMO CATEGORICA**
One important feature is to check that categories exist in both, train and test. Why? It makes no sense to train a model with a category that will no appear in the testing part and viceversa, as we would try to predict something that exceeds out input of the model.

```{r}
obtener_diferencia_categorias <- function(dataframe1, dataframe2) {
  nombres_categorias_df1 <- list()
  nombres_categorias_df2 <- list()
  
  for (col in names(dataframe1)) {
    nombres_categorias_df1[[col]] <- sort(unique(dataframe1[[col]]))
  }
  
  for (col in names(dataframe2)) {
    nombres_categorias_df2[[col]] <- sort(unique(dataframe2[[col]]))
  }
  
  diferencias <- list()
  
  # Obtener las diferencias entre categorías de dataframe1 y dataframe2
  for (col_name in names(dataframe1)) {
    if (col_name %in% names(dataframe2)) {
      diff <- setdiff(nombres_categorias_df1[[col_name]], nombres_categorias_df2[[col_name]])
      if (length(diff) > 0) {
        diferencias[[col_name]] <- diff
      }
    } else {
      diferencias[[col_name]] <- nombres_categorias_df1[[col_name]]
    }
  }
  
  return(diferencias)
}

# Ejemplo de uso
# Supongamos que tienes dos dataframes: 'dataframe_1' y 'dataframe_2'
# Llama a la función y pasa los dataframes como argumentos
categorias_diferentes <- obtener_diferencia_categorias(train[,33:65], test[,32:64])

# Muestra los nombres de las categorías únicas presentes en dataframe_1 pero no en dataframe_2
print("Nombres de categorías presentes en dataframe_1 pero no en dataframe_2:")
print(categorias_diferentes)
```

Once with have the categories that differ, we will replace in the dataset that contains the category that does not occur in the other dataset for the mode of the variable, and delate the category.

```{r}
# HouseStyle
train$HouseStyle <- replace(train$HouseStyle, train$HouseStyle == "2.5Fin", "1Story")
train$HouseStyle <- droplevels(train$HouseStyle, exclude= "2.5Fin")

# Exterior1st
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "ImStucc", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude =  "ImStucc")
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "Stone", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude = "Stone")

# Exterior2st
train$Exterior2nd <- replace(train$Exterior2nd, train$Exterior2nd == "Other", "VinylSd")
train$Exterior2nd <- droplevels(train$Exterior2nd, exclude =  "Other")

# Electrical
train$Electrical <- replace(train$Electrical, train$Electrical == "Mix", "SBrKr")
train$Electrical <- droplevels(train$Electrical, exclude =  "Mix")

# MSSubClass
test$MSSubClass <- replace(test$MSSubClass, test$MSSubClass == '150', '20')
test$MSSubClass <- droplevels(test$MSSubClass, exclude =  '150')
```

## Numeric data handling (outliers and scaling):

Once we have categorical variables in order, we should focus our attention on quantitative data. 

```{r}
#We extract a dataset only with quantitative data (34 variables)
num_train_dataset <- train %>% select_if(is.numeric)
num_test_dataset <- test %>% select_if(is.numeric)
```

We will focus first on the outliers, observations that are far away from the regression line (*Meter sino una def y aprovechar para bibliografía*). They do not affect the estimated model, unless the observation has high leverage. It tends to increase the variability among the variable affecting the negatively the variability explained by the model, making it become worse. 
Comparing the maximum values and the means of some of the variables we suspect the existence of outliers due to the huge differences between them. 

```{r}
summary(num_train_dataset)
summary(num_test_dataset)


#cooks.distance() #Creo que no nos sirve, probar con esto: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/influence.measures

#Para el scaling alguien que entienda que mire los graficos de abajo. Yo creo que alguna hay que hacer log.
```

# Exploratory Analysis

In the first part, we will carry out a descriptive analysis of several variables to understand them better. Graphs will help us to reach this goal, histograms for continuous variables and box-plots for categorical variables.

It would be useful to analyse the categorical variables, seeing most popular categories for each variables.

**HACER BOXPLOTS DE LAS QUE VAYAMOS A EXPLICAR**

```{r}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen", "lightblue")

#1 is sale price
for (i in 2:13) {
  col_name <- colnames(num_train_dataset)[i]
  boxplot(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 1], 
       breaks = 5)
}
```

```{r}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen","lightblue")

for (i in 14:25) {
  col_name <- colnames(num_train_dataset)[i]
  boxplot(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 13],
       breaks = 5)
}
```

```{r}
par(mfrow = c(3, 3))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan")

for (i in 26:31) {
  col_name <- colnames(num_train_dataset)[i]
  boxplot(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 25],
       breaks = 5)
}
```

Same for test:

```{r}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen", "lightblue")

for (i in 1:12) {
  col_name <- colnames(num_test_dataset)[i]
  hist(num_test_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 1], 
       breaks = 5)
}
```

```{r}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen","lightblue")

for (i in 13:24) {
  col_name <- colnames(num_test_dataset)[i]
  hist(num_test_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 12],
       breaks = 5)
}
```

```{r}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan")

for (i in 25:30) {
  col_name <- colnames(num_test_dataset)[i]
  hist(num_test_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 24],
       breaks = 5)
}
```

## GGPairs

Next step is to get the scatter plot for the quantitative features (24 variables) in order to decide which transformation we will apply them (just if it is necessary).

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,1:12],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,c(1,13:24)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

**Negative skeweness**:
-   YearBuilt, YearRemodAdd, GarageYrBlt.
-   For this variables we could apply a ^2 transformation. However after trying this, we observed that there is no any improvement.

**Positive skeweness**:
-   The rest of the variables have positive skewness, so the proper transformation will be a log transformation. After trying to transform these variables, there is no improvement in some of them, so they will remain as the original ones. It is worth to mention that we have done log(X+1) so as to avoid errors with zero values. We have applied it to: BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, GarageArea, WoodDeckSF and OpenPorchSF.

**Others**
-   YrSold;-> This variable has seasonality, there are some years where more sales occur. This makes sense as could depend on the economic cycles. *Buscar tranformación*.

```{r warning=FALSE, message=FALSE,echo=FALSE}
library(psych)
library(pander)
skewness <- round(skew(train[,1:21]), 1)
result_df <- data.frame(variable = colnames(train[,1:21]), skewness = skewness)
pander(result_df)
```


```{r}
# Check if the quantitative feature has a cero or not. This will influence our choice for the traanformation
counter <- 0
for (i in head(names(train), 21)) {
  if (any(!is.na(train[[i]]) & train[[i]] == 0)) {
    counter <- counter + 1
    print(paste("Column '", i, "' has at least one zero value.", sep = ""))
  }
}
```

```{r apply_transformations}
# Positive skewness
# 1/x because there are severe skewness
train[,c(3,11)] <- 1/train[,c(3,11)]
# log
train[,c(2)] <- log(train[,c(2)])
#log+1 because there are 0 values on that feature
train[,c(7,8,9,15,16,17)] <- log(train[,c(7,8,9,15,16,17)]+1)
```

In order to check if we achive the normality distributions on the variables thanks to the transformations apply, we plot again the scatters plots.

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,1:12],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,c(1,13:24)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

## Correlation
To see which the quantitative features that strongly affect home price, checking the correlation may be an interesting idea.

```{r, fig.height=7,fig.width=7}
correlation_matrix <- cor(num_train_dataset,use='pairwise.complete.obs')
rownames(correlation_matrix) <- colnames(correlation_matrix) <- 1:ncol(correlation_matrix)
corrplot(correlation_matrix, method = "color", tl.col = "black", title="Correlation Matrix",tl.srt = 0,tl.cex = 0.7, type="upper")
```

The only variable that are negatively correlated with house price are EnclosedPorch and KitchenAbvGr, but as the correlation of both is quite small we will not worry to much. 
The variables that are highly correlated are:
-YearBuild:
-YearRemodAdd:
-TotalBsmtSF:
-X1stFloorSF:
-GrLivArea:
-GarageArea:
-FullBath:
-GarageCars:

(Fireplaces, GarageYrBuild)

Nonetheless, correlation has other important use, help to find multicollinearity among variables.


How to deal with multicolinearity?: (*Slides T2 o T4- QQPlot*)
-Ignore it
-Use antoher estimator other than Least Square
-Get rid of these redundant variables using variable selection techniques.

##ANOVA:


#Model:

```{r}
train$GarageCond <- factor(train$GarageCond, levels=levels(train$GarageCond)[c(4,5,2,6,3,1)])
```

```{r}
model1 <- lm(SalePrice~LotFrontage + LotArea + YearBuilt + dummy_RemodYear:YearRemodAdd + dummy_masvnr:MasVnrArea+dummy_masvnr:MasVnrType+ BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + X1stFlrSF + X2ndFlrSF + GrLivArea +dummy_garage:GarageCars+dummy_garage:GarageYrBlt+dummy_garage:GarageArea+dummy_garage:GarageType+dummy_garage:GarageFinish+dummy_garage:GarageCond+ WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch + YrSold + BsmtFullBath + BsmtHalfBath + FullBath+ HalfBath + BedroomAbvGr+ KitchenAbvGr + TotRmsAbvGrd+ Fireplaces + MoSold + MSSubClass+ MSZoning + LotShape + LandContour + LotConfig + Neighborhood + BldgType + HouseStyle + OverallQual + OverallCond + RoofStyle + Exterior1st + Exterior2nd + ExterQual + ExterCond + Foundation + BsmtQual + BsmtExposure + BsmtFinType1 + BsmtFinType2 + HeatingQC + Electrical + KitchenQual+ Functional+ FireplaceQu,data=train)

#summary(model1)
```

```{r}
modelo1aic <- stepAIC(model1,direction="both", trace=FALSE)
summary(modelo1aic)
model2 <- lm(formula(modelo1aic),data=train)
```

*Comparar el reduce model with full one using ANOVA - FALTA INTERPRETAR*

```{r}
confint(model2,level=0.95)
```

If 0 is not in the interval is significative

```{r}
anova(model2,model1)
```
Interpretation (*FALTA*): Model without so many variables fits better. The RSS decrease much, from 0.68 to 0.58.

*Interacciones entre categoricas y numericas*

```{r}
# Loop para que haga las interacciones entre todas las categoricas y numericas
for i in 
```


```{r}
plot(modelo1aic$residuals)
shapiro.test(modelo1aic$residuals)
qqnorm(residuals(modelo1aic))
qqline(residuals(modelo1aic))
```

```{r}
resact <- data.frame(residual = modelo1aic$residuals, fitted = modelo1aic$fitted.values)

resact %>% ggplot(aes(fitted, residual)) + geom_point() + geom_smooth() + geom_hline(aes(yintercept = 0)) + 
    theme(panel.grid = element_blank(), panel.background = element_blank())
```

# Hetereosdacity

```{r, cache=FALSE, message=FALSE,warning=FALSE}
resact %>% ggplot(aes(fitted, residual)) + geom_point() + theme_light() + geom_hline(aes(yintercept = 0))

```

```{r, cache=FALSE, message=FALSE,warning=FALSE}
bptest(modelo1aic)
```

```{r, cache=FALSE, message=FALSE,warning=FALSE}
vif(model2)
```

## Correlation matrix (GGPairs)

```{r}
ggcorr(num_train_dataset, label = TRUE, label_size = 2, hjust = 1, digits =3,layout.exp = 2)
```




##Variable selection:
Tema 3




