---
title: "Modelling Competition: House Prices"
author: 
- Roberto Alvarez Llordachs
- Javier Goñi Artieda
- Marcos Crespo Díaz
- Gabriela Levenfeld Sabau
- Sofía Gianelli Nan
- Miguel Díaz-Plaza Cabrera
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    toc_depth: 4
    number_sections: yes
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE, cache=TRUE)
```

# Introduction

House prices have sparked lot of debate recently in our society. Currently, this topic has aroused controversy on the grounds that has been a cornerstone in 2008 crisis. Were buyers to decide a model for setting the selling price, which factors would be included and what would be their weight?

The goal of this project is to create a effective price prediction model.

# Libraries

```{r, message=FALSE, warning=FALSE}
library(skimr)
library(dplyr)
library(tidyverse)
library(caret)
library(plotly)
library(data.table)
library(GGally)
library(tidymodels)
library(car)
library(scales)
library(MASS)
library(lmtest)
library(base)
library(corrplot)
```

# Data Preparation

First step would be to load both databases, test and test.

```{r}
train_original <- read.csv("train.csv", sep=",")
test_original <- read.csv("test.csv", sep=",")
```

```{r}
train <- train_original
test <- test_original 
```


We are going to work with the target variable (SalePrice) in log

```{r}
train$SalePrice<-log(train$SalePrice)
```

## We make EDA (Exploratory Data Analysis) of the test set

We are only going to work with the test set due to the test set is for studying the accuracy of the model.

```{r}
skim(train) 
skim(test)
#Presents results for every column; the statistics it provides depend on the class of the variable
```


```{r}
# Ordenar las columnas por tipo de variable (Las 25 primeras son continuas)
train <- train[,c(1,81,4,5,20,21,27,35,37,38,39,44,45,46,47,60,63,67,68,69,70,71,72,76,78,48,49,50,51,52,53,55,57,62,77,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,22,23,24,25,26,28,29,30,31,32,33,34,36,40,41,42,43,54,56,58,59,61,64,65,66,73,74,75,79,80)]

test <- test[,c(1,4,5,20,21,27,35,37,38,39,44,45,46,47,60,63,67,68,69,70,71,72,76,78,48,49,50,51,52,53,55,57,62,77,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,22,23,24,25,26,28,29,30,31,32,33,34,36,40,41,42,43,54,56,58,59,61,64,65,66,73,74,75,79,80)]
```

ID columns is irrelevant so we can delete it. The number of the observation is given by the row of each dataset.

```{r}
# Eliminate the ID columns
train <- train[,-1]
test <- test[,-1]
```

## Missing value problem:

Looking at the dataset, we observe that some of the variables have great deal of NA values. Therefore, it may interesting to quantify these numbers and later think on future actions.

```{r}
# Counting NA: train and express in %
Number_NA_train<-(colSums(is.na(train))/nrow(train))*100
sort(Number_NA_train,decreasing = TRUE)
```

```{r}
# Counting NA: test and express in %
Number_NA_test<-(colSums(is.na(test))/nrow(test))*100
sort(Number_NA_test,decreasing = TRUE)
```

Acoording to the high percentage of NA, we are going to eliminate the variables "PoolQC" and the associated numerical one, "PoolArea". The same occur with "MiscFeature" and "MiscVal" (numerical), "Fence" and "Alley". 

Other variables are going to be deleted due to the great deal of values in a single category, which would imply that the rest of the categories are seen as outliers. This is the case of: Utilities, Street, Condition2, and PavedDrive.

Furthermore, we delete some variables as we consider to have the similar meaning, those that have Qual and Cond:
  
  - GarageQual and GarageCond, deleting GarageQual.

Finally, some numerical variables have for most of their observations (>90%) the same value, so for the same reason are going to be deleted. This is LowQualFinSF. 

```{r}
train <- train[,-c(13,22,23,37,38,73,75,76,77,78,41,46)]
test <- test[,-c(12,21,22,36,37,72,74,75,76,77,40,45)]
```

What should we then do this those variables that have more missing values than existing ones? Maintaining them can we a problem as makes no sense to substitute missing values with an statistic of the remaining, such as the mean, as the latter would not be representative. 
However, NA values have a meaning in most of the cases where its percentage is high. For example, in PoolQC it means that the house does not have pool, in Alley means that the house no alley access... Consequently, we should change NA for None so that they will later appear when displaying these variables values.

```{r}
train$BsmtFinType1 <- ifelse(is.na(train$BsmtFinType1),"None",train$BsmtFinType1)
train$BsmtCond <- ifelse(is.na(train$BsmtCond),"None",train$BsmtCond)
train$BsmtQual <- ifelse(is.na(train$BsmtQual),"None",train$BsmtQual)
train$BsmtFinType2 <- ifelse(is.na(train$BsmtFinType2),"None",train$BsmtFinType2)
train$BsmtExposure <- ifelse(is.na(train$BsmtExposure),"None",train$BsmtExposure)
train$FireplaceQu <- ifelse(is.na(train$FireplaceQu),"None",train$FireplaceQu)
train$GarageCond <- ifelse(is.na(train$GarageCond),"None",train$GarageCond)
train$GarageFinish <- ifelse(is.na(train$GarageFinish),"None",train$GarageFinish)
train$GarageType <- ifelse(is.na(train$GarageType),"None",train$GarageType)
train$MasVnrType <- ifelse(is.na(train$MasVnrType),"None",train$MasVnrType)
train$GarageArea <- ifelse(is.na(train$GarageArea),0,train$GarageArea)
train$GarageCars <- ifelse(is.na(train$GarageCars),0,train$GarageCars)
train$GarageYrBlt <- ifelse(is.na(train$GarageYrBlt),0,train$GarageYrBlt)
train$MasVnrArea <- ifelse(is.na(train$MasVnrArea),0,train$MasVnrArea)
test$BsmtFinType1 <- ifelse(is.na(test$BsmtFinType1),"None",test$BsmtFinType1)
test$BsmtCond <- ifelse(is.na(test$BsmtCond),"None",test$BsmtCond)
test$BsmtQual <- ifelse(is.na(test$BsmtQual),"None",test$BsmtQual)
test$BsmtFinType2 <- ifelse(is.na(test$BsmtFinType2),"None",test$BsmtFinType2)
test$BsmtExposure <- ifelse(is.na(test$BsmtExposure),"None",test$BsmtExposure)
test$FireplaceQu <- ifelse(is.na(test$FireplaceQu),"None",test$FireplaceQu)
test$GarageCond <- ifelse(is.na(test$GarageCond),"None",test$GarageCond)
test$GarageFinish <- ifelse(is.na(test$GarageFinish),"None",test$GarageFinish)
test$GarageType <- ifelse(is.na(test$GarageType),"None",test$GarageType)
test$MasVnrType <- ifelse(is.na(test$MasVnrType),"None",test$MasVnrType)
test$GarageArea <- ifelse(is.na(test$GarageArea),0,test$GarageArea)
test$GarageCars <- ifelse(is.na(test$GarageCars),0,test$GarageCars)
test$GarageYrBlt <- ifelse(is.na(test$GarageYrBlt),0,test$GarageYrBlt)
test$MasVnrArea <- ifelse(is.na(test$MasVnrArea),0,test$MasVnrArea)
```

Checking Garage variables we suspect that the NA correspond to not having a garage. When the variable is categorical we change it for None, but the problem comes when it is numerical, as imputing 0 for the missing values makes no sense. Numerical variables associated with garage, such as GarageArea or GarageCars have a value of 0 when there is no garage, which as mentioned, makes no sense. Therefore, we will create a dummy variable for first checking if the house has or not garage, so having missing values in GarageYrBuild would not cause a problem. The creation of a dummy variable, allow us to transform the 0 values of GarageArea and GarageCars to NA´s, so as not to affect the mean, median... 

Same process would be applied to other variables, such as MasVnrArea, MasVnrType.

We will fix the missing values in the test data, changing for the mode in categorical variables and for the median in numerical ones.

```{r}
train$Electrical <- ifelse(is.na(train$Electrical),"SBrkr",train$Electrical) 
test$SaleType <- ifelse(is.na(test$SaleType),"WD",test$SaleType)
test$KitchenQual <- ifelse(is.na(test$KitchenQual),"TA",test$KitchenQual)
test$Exterior2nd <- ifelse(is.na(test$Exterior2nd),"VinylSd",test$Exterior2nd)
test$Exterior1st <- ifelse(is.na(test$Exterior1st),"VinylSd",test$Exterior1st)
test$TotalBsmtSF<- ifelse(is.na(test$TotalBsmtSF),median(na.omit(test$TotalBsmtSF)),test$TotalBsmtSF) #numeric
test$BsmtUnfSF<- ifelse(is.na(test$BsmtUnfSF),median(na.omit(test$BsmtUnfSF)),test$BsmtUnfSF) #numeric
test$BsmtFinSF2<- ifelse(is.na(test$BsmtFinSF2),median(na.omit(test$BsmtFinSF2)),test$BsmtFinSF2) #numeric
test$BsmtFinSF1<- ifelse(is.na(test$BsmtFinSF1),median(na.omit(test$BsmtFinSF1)),test$BsmtFinSF1) #numeric
test$BsmtFullBath<- ifelse(is.na(test$BsmtFullBath),median(na.omit(test$BsmtFullBath)),test$BsmtFullBath) #discrete
test$BsmtHalfBath<- ifelse(is.na(test$BsmtHalfBath),median(na.omit(test$BsmtHalfBath)),test$BsmtHalfBath) #discrete
test$Functional<- ifelse(is.na(test$Functional),"Typ",test$Functional)
test$MSZoning<- ifelse(is.na(test$MSZoning),"RL",test$MSZoning)
```

It is worth to mention the case of LotFrontage, as the amount of missing values is high, special attention should be paid. Instead of computing just the median of the data, we have used a multivariate imputation method, so that we have make the median based on other attribute that is related to this variable, such as the neighborhood, with the idea that in the same neighborhood the LotFrontage would be similar.
 
```{r frontage}
# Calcular la mediana por grupo utilizando tapply
medianas_por_grupo_train <- tapply(train$LotFrontage, train$Neighborhood, FUN = function(x) median(x, na.rm = TRUE))
medianas_por_grupo_test <- tapply(test$LotFrontage, test$Neighborhood, FUN = function(x) median(x, na.rm = TRUE))

# Sustituir NA por la mediana correspondiente por grupo
train$LotFrontage <- ifelse(is.na(train$LotFrontage), medianas_por_grupo_train[match(train$Neighborhood,names(medianas_por_grupo_train))],
                                   train$LotFrontage)

test$LotFrontage <- ifelse(is.na(test$LotFrontage), medianas_por_grupo_test[match(test$Neighborhood,names(medianas_por_grupo_test))],
                                   test$LotFrontage)

```

```{r}
#Inicialmente tenemos 259 missing values.
sum(is.na(train$LotFrontage))
sum(is.na(test$LotFrontage))
# Ahora ya tenemos 0 missing values
```

Additionally we see that YearRemodAdd, has construction date if no remodeling or additions have been made. For us that makes no sense, we will create a dummy variable if the house have been remodel and include NA´s when no remodel has been made, so as not to affect the mean, minimum and maximum.

Other important action in data preprocessing is to check if there are duplicated categories referring to the same thing. That was done manually and it seems that no duplicated categories exist.

Moreover, we should check if there are duplicated observations, using the duplicated functions.

```{r duplicates}
sum(duplicated(train))
sum(duplicated(test))
```

## Handle categorical variables:

Next, the goal is to modify categorical variables as a factor so as to work with them.

```{r}
train <- train %>% mutate_if(~is.character(.), ~as.factor(.))
#Nonetheless not all categorical variables are character defined, we modify using factor those who aren´t
train$OverallQual <- factor(train$OverallQual)
train$OverallCond <- factor(train$OverallCond)
train$MSSubClass <- factor(train$MSSubClass)

test <- test %>% mutate_if(~is.character(.), ~as.factor(.))
#Nonetheless not all categorical variables are character defined, we modify using factor those who aren´t
test$OverallQual <- factor(test$OverallQual)
test$OverallCond <- factor(test$OverallCond)
test$MSSubClass <- factor(test$MSSubClass)
```

**CON OVERALL's NO SABEMOS AUN SI TRATAR COMO NUMERICA O COMO CATEGORICA**
One important feature is to check that categories exist in both, train and test. Why? It makes no sense to train a model with a category that will no appear in the testing part and viceversa, as we would try to predict something that exceeds out input of the model.

```{r}
obtener_diferencia_categorias <- function(dataframe1, dataframe2) {
  nombres_categorias_df1 <- list()
  nombres_categorias_df2 <- list()
  
  for (col in names(dataframe1)) {
    nombres_categorias_df1[[col]] <- sort(unique(dataframe1[[col]]))
  }
  
  for (col in names(dataframe2)) {
    nombres_categorias_df2[[col]] <- sort(unique(dataframe2[[col]]))
  }
  
  diferencias <- list()
  
  # Obtener las diferencias entre categorías de dataframe1 y dataframe2
  for (col_name in names(dataframe1)) {
    if (col_name %in% names(dataframe2)) {
      diff <- setdiff(nombres_categorias_df1[[col_name]], nombres_categorias_df2[[col_name]])
      if (length(diff) > 0) {
        diferencias[[col_name]] <- diff
      }
    } else {
      diferencias[[col_name]] <- nombres_categorias_df1[[col_name]]
    }
  }
  
  return(diferencias)
}

# Ejemplo de uso
# Supongamos que tienes dos dataframes: 'dataframe_1' y 'dataframe_2'
# Llama a la función y pasa los dataframes como argumentos
categorias_diferentes <- obtener_diferencia_categorias(train[,32:68], test[,31:67])

# Muestra los nombres de las categorías únicas presentes en dataframe_1 pero no en dataframe_2
print("Nombres de categorías presentes en dataframe_1 pero no en dataframe_2:")
print(categorias_diferentes)
```

Once with have the categories that differ, we will replace in the dataset that contains the category that does not occur in the other dataset for the mode of the variable, and delete the category.

```{r}
# HouseStyle
train$HouseStyle <- replace(train$HouseStyle, train$HouseStyle == "2.5Fin", "1Story")
train$HouseStyle <- droplevels(train$HouseStyle, exclude= "2.5Fin")
# Roofmatl
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "ClyTile", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude =  "ClyTile")
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "Membran", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude =  "Membran")
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "Metal", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude = "Metal")
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "Roll", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude =  "Roll")
# Exterior1st
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "ImStucc", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude =  "ImStucc")
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "Stone", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude = "Stone")
# Exterior2st
train$Exterior2nd <- replace(train$Exterior2nd, train$Exterior2nd == "Other", "VinylSd")
train$Exterior2nd <- droplevels(train$Exterior2nd, exclude =  "Other")
# Heating
train$Heating <- replace(train$Heating, train$Heating == "Floor", "GasA")
train$Heating <- droplevels(train$Heating, exclude =  "Floor")
train$Heating <- replace(train$Heating, train$Heating == "OthW", "GasA")
train$Heating <- droplevels(train$Heating, exclude =  "OthW")
# Electrical
train$Electrical <- replace(train$Electrical, train$Electrical == "Mix", "SBrkr")
train$Electrical <- droplevels(train$Electrical, exclude =  "Mix")
# MSSubClass
test$MSSubClass <- replace(test$MSSubClass, test$MSSubClass == '150', '20')
test$MSSubClass <- droplevels(test$MSSubClass, exclude =  '150')
```

## Numeric data handling (outliers and scaling):

Once we have categorical variables in order, we should focus our attention on quantitative data. 

```{r}
#We extract a dataset only with quantitative data (34 variables)
num_train_dataset <- train %>% select_if(is.numeric)
num_test_dataset <- test %>% select_if(is.numeric)
```

We will focus first on the outliers, observations that are far away from the regression line (*Meter sino una def y aprovechar para bibliografía*). They do not affect the estimated model, unless the observation has high leverage. It tends to increase the variability among the variable affecting the negatively the variability explained by the model, making it become worse. 
Comparing the maximum values and the means of some of the variables we suspect the existence of outliers due to the huge differences between them. 

```{r}
summary(num_train_dataset)
summary(num_test_dataset)


#cooks.distance() #Creo que no nos sirve, probar con esto: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/influence.measures

#Para el scaling alguien que entienda que mire los graficos de abajo. Yo creo que alguna hay que hacer log.
```

# Exploratory Analysis

In the first part, we will carry out a descriptive analysis of several variables to understand them better. Graphs will help us to reach this goal, histograms for continuous variables and box-plots for categorical variables.

It would be useful to analyse the categorical variables, seeing most popular categories for each variables.



## GGPairs

Next step is to get the scatter plot for the quantitative features (24 variables) in order to decide which transformation we will apply them (just if it is necessary).

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,1:12],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,c(1,13:24)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,c(1,25:31)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

**Negative skeweness**:

-   YearBuilt, YearRemodAdd, GarageYrBlt.

-   For this variables we could apply a ^2 transformation. However after trying this, we observed that there is no any improvement.

**Positive skeweness**:

-   The rest of the variables have positive skewness, so the proper transformation will be a log transformation. After trying to transform these variables, there is no improvement in some of them, so they will remain as the original ones. It is worth to mention that we have done log(X+1) so as to avoid errors with zero values. We have applied it to: BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, GarageArea, WoodDeckSF and OpenPorchSF.

**Others**

-   YrSold;-> This variable has seasonality, there are some years where more sales occur. This makes sense as could depend on the economic cycles. *Buscar tranformación*.

```{r warning=FALSE, message=FALSE,echo=FALSE}
library(psych)
library(pander)
skewness <- round(skew(train[,1:21]), 1)
result_df <- data.frame(variable = colnames(train[,1:21]), skewness = skewness)
pander(result_df)
```

```{r}
# Check if the quantitative feature has a cero or not. This will influence our choice for the traanformation
counter <- 0
for (i in head(names(train), 21)) {
  if (any(!is.na(train[[i]]) & train[[i]] == 0)) {
    counter <- counter + 1
    print(paste("Column '", i, "' has at least one zero value.", sep = ""))
  }
}
```

```{r apply_transformations}
# Positive skewness
# 1/x because there are severe skewness
#train[,c(3,11)] <- 1/train[,c(3,11)]
# sqrt negative
train[,c(14)] <- sqrt(max(train[,c(14)])+1-train[,c(14)])
# sqrt positive
train[,c(9)] <- sqrt(train[,c(9)])
# log
train[,c(2,11,13,14)] <- log(train[,c(2,11,13,14)])
# log+1 because there are 0 values on that feature
train[,c(3,6,7,10,12,16,17,22,24,25,27,29)] <- log(train[,c(3,6,7,10,12,16,17,22,24,25,27,29)]+1)
```

In order to check if we archive the normality distributions on the variables thanks to the transformations apply, we plot again the scatters plots.

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,1:12],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,c(1,13:24)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE}
ggpairs(train[,c(1,25:31)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```


**NO LAS INCLUIMOS EN EL MODELO, TIENEN MUCHOS 0, NOS HEMOS DADO CUENTA AHORA.**

```{r}
train<-train[,-c(8,18,19,20,23)]
```

- BsmtFinSF2

- EnclosedPorch

- X3SsnPorch

- ScreenPorch

- BsmtHalfBath


## Correlation

To see which the quantitative features that strongly affect home price, checking the correlation may be an interesting idea.

```{r, fig.height=7,fig.width=7}
correlation_matrix <- cor(train[1:26],use='pairwise.complete.obs')
rownames(correlation_matrix) <- colnames(correlation_matrix) <- 1:ncol(correlation_matrix)
corrplot(correlation_matrix, method = "color", tl.col = "black", title="Correlation Matrix",tl.srt = 0,tl.cex = 0.7, type="upper")
```

There is correlation between:

- GarageCars - GarageArea

- GarageCars - GarageYrBlt

- TotRmsAbvGrd - GrLivArea

# Model:

Cambiamos el orden de los factores, a que el base sea "None".

```{r}
train$GarageCond <- factor(train$GarageCond, levels=levels(train$GarageCond)[c(4,5,2,6,3,1)])
```

Separamos en TRAIN_TRAIN (70%) Y TRAIN_VALIDATION (30%)

```{r}
set.seed(33)
samplesize <- round(0.7 * nrow(train), 0)
index <- sample(seq_len(nrow(train)), size = samplesize)

train_train <- train[index, ]
train_validation  <- train[-index, ]
```


```{r}
model1 <- lm(SalePrice~.,data=train_train)
```

If we were to try to reduce the number of variables using vif, we realize that it returns an error as some variables have a correlation of 1 between them. However, this does not have to do between numeric variables, as we can calculate the correlation values between them and see that none of them is zero:
{r, cache=FALSE, message=FALSE,warning=FALSE}
ggcorr(num_train_dataset, label = TRUE, label_size = 2.9, hjust = 1, digits =3,layout.exp = 2)


Therefore, the problem is with categorical variables. For getting the values that are causing problems we use the alias function.

````{r}
alias(model1)
```

We check those variables where the correlation between them has either 1 or -1. First we see, correlation for KitchenAbvGr and: MsSubClass and BldgType, so we delete KitchenAbvGr. Computing again the alias we see correlation for MsSubClass and HouseStyle and delete the latter. 
Our criteria for choosing the variable to delete is based on the description and the variable and the intuition on which one affect more to the sale price. Another important criterion is try to delete those variables that has ones with many variables. 
With the alias function we delete GarageCond, BldgType, Exterior2nd, BsmtQual, BsmtExposure, BsmtFinType1 and GarageType.

```{r}
model2 <- lm(SalePrice~.-GarageCond -BldgType -Exterior2nd -BsmtQual -BsmtExposure -BsmtFinType1 -GarageType,data=train_train)
alias(model2)
```

The following step is to use step AIC.

```{r}
model2aic <- stepAIC(model2,direction="both", trace=FALSE)
summary(model2aic)
```

## Multicoliniarity 

**Checking the VIF**

```{r, cache=FALSE, message=FALSE,warning=FALSE}
vif(model2aic)
```

We delete `TotalBsmtSF` . GVIF^(1/(2*Df))=8.489150 . We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif1<- lm(formula = SalePrice ~ LotArea + YearBuilt + YearRemodAdd + 
    BsmtFinSF1 + BsmtUnfSF + X2ndFlrSF + GrLivArea + 
    GarageYrBlt + GarageArea + WoodDeckSF + YrSold + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtCond + Heating + 
    CentralAir + KitchenQual + Functional + GarageFinish + SaleCondition, data = train_train)
vif(modelvif1)
```

We remove `X2ndFlrSF` the GVIF^(1/(2*Df))=4.533315. We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif2<- lm(formula = SalePrice ~ LotArea + YearBuilt + YearRemodAdd + 
    BsmtFinSF1 + BsmtUnfSF + GrLivArea + 
    GarageYrBlt + GarageArea + WoodDeckSF + YrSold + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtCond + Heating + 
    CentralAir + KitchenQual + Functional + GarageFinish + SaleCondition, data = train_train)
vif(modelvif2)
```


We remove `YearBuilt` the GVIF^(1/(2*Df))=4.076025. We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif3<- lm(formula = SalePrice ~ LotArea + YearRemodAdd + 
    BsmtFinSF1 + BsmtUnfSF + GrLivArea + 
    GarageYrBlt + GarageArea + WoodDeckSF + YrSold + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtCond + Heating + 
    CentralAir + KitchenQual + Functional + GarageFinish + SaleCondition, data = train_train)
vif(modelvif3)
```

We remove `GarageYrBlt` the GVIF^(1/(2*Df))=3.937849. We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif4<- lm(formula = SalePrice ~ LotArea + YearRemodAdd + 
    BsmtFinSF1 + BsmtUnfSF + GrLivArea + 
    GarageArea + WoodDeckSF + YrSold + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtCond + Heating + 
    CentralAir + KitchenQual + Functional + GarageFinish + SaleCondition, data = train_train)
vif(modelvif4)
```

Ya no hay multicolinealidad.

```{r}
summary(modelvif4)
```


## Interaction between categorical and numeric variables

The following code, give us the p-value between 2 models. It also creates a file to storage the final results, because this is a process that is a little bit time-consuming. After it will help us to check and compare the p-values for significant iterations.

With the next 2 chunk the idea is to find the most significant interaction between variables so we can incorporate them into the final model.
Usually, we mark as a significant interaction when p<0.05. However, because we have obtained 30 interactions, we decided to lower such value into 0.005. With this setting (p<0.005) we just find 1 relevant interaction.


```{r}
dependent_variable <- "SalePrice"
name_vars <- c('LotArea' , 'YearRemodAdd' , 'BsmtFinSF1' , 
    'BsmtUnfSF' , 'GrLivArea' , 'GarageArea' , 'WoodDeckSF' , 'YrSold' , 
    'BsmtFullBath' , 'FullBath' , 'HalfBath' , 'BedroomAbvGr' , 'Fireplaces' , 
    'GarageCars' , 'MoSold' , 'MSSubClass' , 'MSZoning' , 'LotConfig' , 
    'LandSlope' , 'Neighborhood' , 'Condition1' , 'HouseStyle' , 'OverallQual' , 
    'OverallCond' , 'RoofMatl' , 'Exterior1st' , 'Foundation' , 'BsmtCond' , 
    'Heating' , 'CentralAir' , 'KitchenQual' , 'Functional' , 'GarageFinish' , 
    'SaleCondition')
interaction_matrix <- matrix(0, ncol =  length(name_vars), nrow =  length(name_vars), dimnames = list(name_vars,name_vars))

# Loop through all pairs of variables
for (i in 1:(length(name_vars))) {
  variable1 <- name_vars[i]
  for (j in (i): length(name_vars)) {
    variable2 <- name_vars[j]
    if (variable1 != dependent_variable && variable2 != dependent_variable && variable1 != variable2) {
      # Run both models
      lmod_all_vars <- lm(as.formula(paste(dependent_variable, "~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + YrSold + 
    BsmtFullBath + FullBath + HalfBath + BedroomAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtCond + 
    Heating + CentralAir + KitchenQual + Functional + GarageFinish + 
    SaleCondition +", variable1, ":", variable2)), data = train_train)
      lmod_without_inter <- lm(as.formula(paste(dependent_variable, "~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + YrSold + 
    BsmtFullBath + FullBath + HalfBath + BedroomAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtCond + 
    Heating + CentralAir + KitchenQual + Functional + GarageFinish + 
    SaleCondition -", variable1, ":", variable2)), data = train_train)
    
      anova_result <- anova(lmod_all_vars, lmod_without_inter)
      interaction_matrix[variable1, variable2] <- anova_result[2, "Pr(>F)"]
    }
  }
}

# Print the significant interaction matrix
print(interaction_matrix)
# Save the significant interaction matrix to a CSV file
write.csv(interaction_matrix, file = "anova_matrix_05.csv", row.names = TRUE)
```

The next code, allow to get the interactions with test value lower than 0.05:

```{r}
anova_05 <- read.csv("anova_matrix_05.csv", sep=",")
# Extract row and column names where values are lower than 0.05
significant_interactions <- which(anova_05 < 0.001 & anova_05>0, arr.ind = TRUE)

# Print the significant interactions
for (i in 1:nrow(significant_interactions)) {
  row_index <- rownames(anova_05)[significant_interactions[i, 1]]
  col_index <- colnames(anova_05)[significant_interactions[i, 2]]
  cat(anova_05[row_index,1], ":", col_index, "+")
}
```

+YearRemodAdd : GrLivArea +LotArea : Neighborhood  


```{r}
model_interactions <- lm(formula= SalePrice~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + YrSold + 
    BsmtFullBath + FullBath + HalfBath + BedroomAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtCond + 
    Heating + CentralAir + KitchenQual + Functional + GarageFinish + 
    SaleCondition +YearRemodAdd : GrLivArea +LotArea : Neighborhood , data = train_train)

summary(model_interactions)
```

## Compare reduce model with full one using ANOVA

```{r}
anova(modelvif4, model_interactions)
```

Its better the model with the interactions.

## (MULTICOLINEARITY AGAIN)

```{r}
alias(model_interactions)
```

```{r}
vif(model_interactions)
```


## Linearity

```{r, cache=FALSE, message=FALSE,warning=FALSE}
resact <- data.frame(residual = model_interactions$residuals, fitted = model_interactions$fitted.values)
resact %>% ggplot(aes(fitted, residual)) + geom_point() + geom_smooth() + geom_hline(aes(yintercept = 0)) + 
    theme(panel.grid = element_blank(), panel.background = element_blank())
```

## Normality

Shapiro test

H0 = normality

```{r}
shapiro.test(model_interactions$residuals)
qqnorm(residuals(model_interactions))
qqline(residuals(model_interactions))
```

So data do not follow a normal distribution


```{r}
confint(model_interactions,level=0.95)
```

If 0 is not in the interval is significative


## Outliers


```{r}
par(mfrow=c(2,2))
plot(model_interactions)
```

```{r}
# Crear el gráfico de cuantiles normales de los residuos
residual_qqnorm <- qqnorm(model_interactions$residuals)

# Obtener los índices de los valores atípicos
outliers_indices <- which(abs(model_interactions$residuals) > 4 * sd(model_interactions$residuals))

# Acceder a los valores atípicos
outliers <- model_interactions$residuals[outliers_indices]
```

```{r}
model_outliers<- lm(formula = SalePrice ~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + YrSold + 
    BsmtFullBath + FullBath + HalfBath + BedroomAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtCond + 
    Heating + CentralAir + KitchenQual + Functional + GarageFinish + 
    SaleCondition + YearRemodAdd:GrLivArea + LotArea:Neighborhood, data = train_train[-outliers_indices,])
summary(model_outliers)
```

The model R^2 improves and the residual standard error decrease from 0.10 to 0.08

```{r}
par(mfrow=c(2,2))
plot(model_outliers)
```


```{r}
shapiro.test(model_outliers$residuals)
```

As we can not see a signicative improvement in the model, we remain these outliers in the model. We continue with `modelovif1_outliers`

## Hetereosdacity

```{r, cache=FALSE, message=FALSE,warning=FALSE}
resact %>% ggplot(aes(fitted, residual)) + geom_point() + theme_light() + geom_hline(aes(yintercept = 0))
```

```{r, cache=FALSE, message=FALSE,warning=FALSE}
bptest(model_outliers)
```

# Model tunning

Since the assumptions are not satisfied we need to explore possible transformations of the response variable:


```{r, cache=FALSE, message=FALSE,warning=FALSE}
modelovif1_outliers.boxcox=a=boxcox(modelovif1_outliers,lambda = seq(-10, 2, length.out = 10))
modelovif1_outliers.boxcox$x[which.max(modelovif1_outliers.boxcox$y)]
```

# Model Performance

```{r, cache=FALSE, message=FALSE,warning=FALSE}
# RMSE of train dataset
RMSE(model_outliers$fitted.values, train_validation$SalePrice)
```


The accuracy of the model in predicting the car price is measured with RMSE, with training data has RMSE of 1980.464 and testing data has RMSE of 3081.996, suggesting that our model may overfit the training dataset.


If we compare with a the model without one of the variables (for example stroke):


````{r, cache=FALSE, message=FALSE,warning=FALSE}

mod0.data4=update(mod3,.~.-stroke )

# RMSE of train dataset
RMSE(pred = (mod0.data4$fitted.values*0.06+1)^(1/0.06), obs = (data_train4$price*0.06+1)^(1/0.06))

# RMSE in the test dataset

mod.data4_pred <- predict(mod0.data4, newdata = data_test4)
RMSE(pred = (mod.data4_pred*0.06+1)^(1/0.06), obs = (data_test4$price*0.06+1)^(1/0.06))

```
