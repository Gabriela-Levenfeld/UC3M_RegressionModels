---
title: "Modelling Competition: House Prices"
author: 
- Roberto Alvarez Llordachs
- Javier Goñi Artieda
- Marcos Crespo Díaz
- Gabriela Levenfeld Sabau
- Sofía Gianelli Nan
- Miguel Díaz-Plaza Cabrera
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    toc_depth: 4
    number_sections: yes
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE, cache=TRUE)
```

# Introduction

House prices have sparked lot of debate recently in our society. Currently, this topic has aroused controversy on the grounds that has been a cornerstone in 2008 crisis. Were buyers to decide a model for setting the selling price, which factors would be included and what would be their weight?

The goal of this project is to create a effective price prediction model, identifying the most important home price attributes.

# Libraries

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(skimr)
library(dplyr)
library(tidyverse)
library(caret)
library(plotly)
library(data.table)
library(GGally)
library(tidymodels)
library(car)
library(scales)
library(MASS)
library(lmtest)
library(base)
library(corrplot)
library(ggplot2)
library(psych)
library(pander)
```

# Data Preparation

First step would be to load both databases, test and test.

```{r}
train_original <- read.csv("train.csv", sep=",")
test_original <- read.csv("test.csv", sep=",")
```

```{r, echo=FALSE}
train <- train_original
test <- test_original 
```

We are going to work with the target variable (SalePrice) in log units.

```{r}
train$SalePrice<-log(train$SalePrice)
```

We are only going to work with the train database because the test set is for studying the accuracy of the model.

```{r, echo=FALSE}
#Small Exploratory Data Analysis
skim(train) 
skim(test)
#Presents results for every column; the statistics it provides depend on the class of the variable
```

```{r, echo=FALSE}
# Ordenar las columnas por tipo de variable (Las 25 primeras son continuas)
train <- train[,c(1,81,4,5,20,21,27,35,37,38,39,44,45,46,47,60,63,67,68,69,70,71,72,76,78,48,49,50,51,52,53,55,57,62,77,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,22,23,24,25,26,28,29,30,31,32,33,34,36,40,41,42,43,54,56,58,59,61,64,65,66,73,74,75,79,80)]

test <- test[,c(1,4,5,20,21,27,35,37,38,39,44,45,46,47,60,63,67,68,69,70,71,72,76,78,48,49,50,51,52,53,55,57,62,77,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,19,22,23,24,25,26,28,29,30,31,32,33,34,36,40,41,42,43,54,56,58,59,61,64,65,66,73,74,75,79,80)]
```

ID columns is irrelevant so we can delete it. The number of the observation is given by the row of each dataset.

```{r delete ID}
train <- train[,-1]
test <- test[,-1]
```

## Missing value problem:

Looking at the dataset, we observe that some of the variables have great deal of NA values. Therefore, it may interesting to quantify these numbers and later think on future actions.

```{r NA count}
# Counting NA: train and express in %
Number_NA_train<-(colSums(is.na(train))/nrow(train))*100
sort(Number_NA_train,decreasing = TRUE)
# Counting NA: test and express in %
Number_NA_test<-(colSums(is.na(test))/nrow(test))*100
sort(Number_NA_test,decreasing = TRUE)
```

What should we do with the variables that have more missing values than existing ones? Maintaining them can we a problem as makes no sense to substitute missing values with an statistic of the remaining, such as the mean, as the latter would not be representative. Consequently, we are going to delete the variables, where the percentage of NA is high (>80%). They are: "PoolQC" and the associated numerical one, "PoolArea", "MiscFeature" and "MiscVal" (numerical), "Fence" and "Alley". 

Other variables are going to be deleted due to the great deal of values in a single category, which would imply that the rest of the categories are seen as outliers. This is the case of: Utilities, Street, Condition2, and PavedDrive.

Furthermore, we consider that two variables have a similar meaning, GarageQual and GarageCond, so we delete one of them, GarageQual.

Finally, checking numerical variables we observe that LowQualFinSF, have most of their observations (>90%) at the same value, so we delete it. 

```{r deleting values}
train <- train[,-c(13,22,23,37,38,73,75,76,77,78,41,46)]
test <- test[,-c(12,21,22,36,37,72,74,75,76,77,40,45)]
```

In most of the cases, NA´s have meaning, it means that the house lack of the object mentioned. For example, in the variable FirePlaceQu, NA means no fireplaces. Those that start with MasVnr, the NA means that we have no Masonry veneer area. (the NA means that the house has no basement, for Bsmt... variables, no garage in the houses where we find a NA in garage...). Consequently, we should change NA for None so that they will later appear when displaying these variables values and 0 for numerical ones. We checked that numerical variables that do not have NAs (GarageArea or GarageCars) have the same percentage of 0´s that we have for NA´s.

```{r replace na, echo=FALSE}
train$BsmtFinType1 <- ifelse(is.na(train$BsmtFinType1),"None",train$BsmtFinType1)
train$BsmtCond <- ifelse(is.na(train$BsmtCond),"None",train$BsmtCond)
train$BsmtQual <- ifelse(is.na(train$BsmtQual),"None",train$BsmtQual)
train$BsmtFinType2 <- ifelse(is.na(train$BsmtFinType2),"None",train$BsmtFinType2)
train$BsmtExposure <- ifelse(is.na(train$BsmtExposure),"None",train$BsmtExposure)
train$FireplaceQu <- ifelse(is.na(train$FireplaceQu),"None",train$FireplaceQu)
train$GarageCond <- ifelse(is.na(train$GarageCond),"None",train$GarageCond)
train$GarageFinish <- ifelse(is.na(train$GarageFinish),"None",train$GarageFinish)
train$GarageType <- ifelse(is.na(train$GarageType),"None",train$GarageType)
train$MasVnrType <- ifelse(is.na(train$MasVnrType),"None",train$MasVnrType)
train$GarageArea <- ifelse(is.na(train$GarageArea),0,train$GarageArea)
train$GarageCars <- ifelse(is.na(train$GarageCars),0,train$GarageCars)
train$GarageYrBlt <- ifelse(is.na(train$GarageYrBlt),0,train$GarageYrBlt)
train$MasVnrArea <- ifelse(is.na(train$MasVnrArea),0,train$MasVnrArea)

test$BsmtFinType1 <- ifelse(is.na(test$BsmtFinType1),"None",test$BsmtFinType1)
test$BsmtCond <- ifelse(is.na(test$BsmtCond),"None",test$BsmtCond)
test$BsmtQual <- ifelse(is.na(test$BsmtQual),"None",test$BsmtQual)
test$BsmtFinType2 <- ifelse(is.na(test$BsmtFinType2),"None",test$BsmtFinType2)
test$BsmtExposure <- ifelse(is.na(test$BsmtExposure),"None",test$BsmtExposure)
test$FireplaceQu <- ifelse(is.na(test$FireplaceQu),"None",test$FireplaceQu)
test$GarageCond <- ifelse(is.na(test$GarageCond),"None",test$GarageCond)
test$GarageFinish <- ifelse(is.na(test$GarageFinish),"None",test$GarageFinish)
test$GarageType <- ifelse(is.na(test$GarageType),"None",test$GarageType)
test$MasVnrType <- ifelse(is.na(test$MasVnrType),"None",test$MasVnrType)
test$GarageArea <- ifelse(is.na(test$GarageArea),0,test$GarageArea)
test$GarageCars <- ifelse(is.na(test$GarageCars),0,test$GarageCars)
test$GarageYrBlt <- ifelse(is.na(test$GarageYrBlt),0,test$GarageYrBlt)
test$MasVnrArea <- ifelse(is.na(test$MasVnrArea),0,test$MasVnrArea)
```

At first we decided to create dummy variables for garage variables, YearRemodAdd and MasVnr variables,as we thought that imputing 0 for the missing values makes no sense in the numerical variables, as distort the means and other measures. The dummy will equal 0 when the house has no garage and 1 when yes, so we would replace the 0´s of numerical variables to NA´s, as it does not matter their value as would be multiplied by 0. However, at the end, when computing the error we were having problems with the NA´s. Therefore, we continue with the above described model.

We will fix the rest of the missing values in the test data, changing for the mode in categorical variables and for the median in numerical ones.

```{r na en test,echo=FALSE}
train$Electrical <- ifelse(is.na(train$Electrical),"SBrkr",train$Electrical) 
test$SaleType <- ifelse(is.na(test$SaleType),"WD",test$SaleType)
test$KitchenQual <- ifelse(is.na(test$KitchenQual),"TA",test$KitchenQual)
test$Exterior2nd <- ifelse(is.na(test$Exterior2nd),"VinylSd",test$Exterior2nd)
test$Exterior1st <- ifelse(is.na(test$Exterior1st),"VinylSd",test$Exterior1st)
test$TotalBsmtSF<- ifelse(is.na(test$TotalBsmtSF),median(na.omit(test$TotalBsmtSF)),test$TotalBsmtSF) #numeric
test$BsmtUnfSF<- ifelse(is.na(test$BsmtUnfSF),median(na.omit(test$BsmtUnfSF)),test$BsmtUnfSF) #numeric
test$BsmtFinSF2<- ifelse(is.na(test$BsmtFinSF2),median(na.omit(test$BsmtFinSF2)),test$BsmtFinSF2) #numeric
test$BsmtFinSF1<- ifelse(is.na(test$BsmtFinSF1),median(na.omit(test$BsmtFinSF1)),test$BsmtFinSF1) #numeric
test$BsmtFullBath<- ifelse(is.na(test$BsmtFullBath),median(na.omit(test$BsmtFullBath)),test$BsmtFullBath) #discrete
test$BsmtHalfBath<- ifelse(is.na(test$BsmtHalfBath),median(na.omit(test$BsmtHalfBath)),test$BsmtHalfBath) #discrete
test$Functional<- ifelse(is.na(test$Functional),"Typ",test$Functional)
test$MSZoning<- ifelse(is.na(test$MSZoning),"RL",test$MSZoning)
```

It is worth to mention the case of LotFrontage, as the amount of missing values is high, special attention should be paid. Instead of computing just the median of the data, we have used a multivariate imputation method, so that we have make the median based on other attribute that is related to this variable, such as the neighborhood, with the idea that in the same neighborhood the LotFrontage would be similar.
 
```{r frontage echo=FALSE}
# Calcular la mediana por grupo utilizando tapply
medianas_por_grupo_train <- tapply(train$LotFrontage, train$Neighborhood, FUN = function(x) median(x, na.rm = TRUE))
medianas_por_grupo_test <- tapply(test$LotFrontage, test$Neighborhood, FUN = function(x) median(x, na.rm = TRUE))

# Sustituir NA por la mediana correspondiente por grupo
train$LotFrontage <- ifelse(is.na(train$LotFrontage), medianas_por_grupo_train[match(train$Neighborhood,names(medianas_por_grupo_train))],
                                   train$LotFrontage)

test$LotFrontage <- ifelse(is.na(test$LotFrontage), medianas_por_grupo_test[match(test$Neighborhood,names(medianas_por_grupo_test))],
                                   test$LotFrontage)

```

```{r check,echo=FALSE}
#Inicialmente tenemos 259 missing values.
sum(is.na(train$LotFrontage))
sum(is.na(test$LotFrontage))
# Ahora ya tenemos 0 missing values
```

Other important action in data preprocessing is to check if there are duplicated categories referring to the same thing. That was done manually and it seems that no duplicated categories exist.

Moreover, we should check if there are duplicated observations, using the duplicated functions. There are not.

```{r duplicates, echo=FALSE}
sum(duplicated(train))
sum(duplicated(test))
```

## Handle categorical variables:

Next, the goal is to modify categorical variables as a factor so as to work with them.

```{r, echo=FALSE}
train <- train %>% mutate_if(~is.character(.), ~as.factor(.))
#Nonetheless not all categorical variables are character defined, we modify using factor those who aren´t
train$OverallQual <- factor(train$OverallQual)
train$OverallCond <- factor(train$OverallCond)
train$MSSubClass <- factor(train$MSSubClass)

test <- test %>% mutate_if(~is.character(.), ~as.factor(.))
#Nonetheless not all categorical variables are character defined, we modify using factor those who aren´t
test$OverallQual <- factor(test$OverallQual)
test$OverallCond <- factor(test$OverallCond)
test$MSSubClass <- factor(test$MSSubClass)
```

One important feature is to check that categories exist in both, train and test. Why? It makes no sense to train a model with a category that will no appear in the testing part and viceversa, as we would try to predict something that exceeds out input of the model.

```{r, echo=FALSE}
obtener_diferencia_categorias <- function(dataframe1, dataframe2) {
  nombres_categorias_df1 <- list()
  nombres_categorias_df2 <- list()
  
  for (col in names(dataframe1)) {
    nombres_categorias_df1[[col]] <- sort(unique(dataframe1[[col]]))
  }
  
  for (col in names(dataframe2)) {
    nombres_categorias_df2[[col]] <- sort(unique(dataframe2[[col]]))
  }
  
  diferencias <- list()
  
  # Obtener las diferencias entre categorías de dataframe1 y dataframe2
  for (col_name in names(dataframe1)) {
    if (col_name %in% names(dataframe2)) {
      diff <- setdiff(nombres_categorias_df1[[col_name]], nombres_categorias_df2[[col_name]])
      if (length(diff) > 0) {
        diferencias[[col_name]] <- diff
      }
    } else {
      diferencias[[col_name]] <- nombres_categorias_df1[[col_name]]
    }
  }
  
  return(diferencias)
}

# Ejemplo de uso
# Supongamos que tienes dos dataframes: 'dataframe_1' y 'dataframe_2'
# Llama a la función y pasa los dataframes como argumentos
categorias_diferentes <- obtener_diferencia_categorias(train[,32:68], test[,31:67])
categorias_diferentes_test <- obtener_diferencia_categorias(test[,31:67],train[,32:68])
# Muestra los nombres de las categorías únicas presentes en dataframe_1 pero no en dataframe_2
print("Nombres de categorías presentes en dataframe_1 pero no en dataframe_2:")
print(categorias_diferentes_test)
```

Once with have the categories that differ, we will replace in the dataset that contains the category that does not occur in the other dataset, the values of those category for the mode of the variable, and delete the category.

```{r, echo=FALSE}
# HouseStyle
train$HouseStyle <- replace(train$HouseStyle, train$HouseStyle == "2.5Fin", "1Story")
train$HouseStyle <- droplevels(train$HouseStyle, exclude= "2.5Fin")
# Roofmatl
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "ClyTile", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude =  "ClyTile")
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "Membran", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude =  "Membran")
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "Metal", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude = "Metal")
train$RoofMatl <- replace(train$RoofMatl, train$RoofMatl == "Roll", "CompShg")
train$RoofMatl <- droplevels(train$RoofMatl, exclude =  "Roll")
# Exterior1st
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "ImStucc", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude =  "ImStucc")
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "Stone", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude = "Stone")
# Exterior2st
train$Exterior2nd <- replace(train$Exterior2nd, train$Exterior2nd == "Other", "VinylSd")
train$Exterior2nd <- droplevels(train$Exterior2nd, exclude =  "Other")
# Heating
train$Heating <- replace(train$Heating, train$Heating == "Floor", "GasA")
train$Heating <- droplevels(train$Heating, exclude =  "Floor")
train$Heating <- replace(train$Heating, train$Heating == "OthW", "GasA")
train$Heating <- droplevels(train$Heating, exclude =  "OthW")
# Electrical
train$Electrical <- replace(train$Electrical, train$Electrical == "Mix", "SBrkr")
train$Electrical <- droplevels(train$Electrical, exclude =  "Mix")
# MSSubClass
test$MSSubClass <- replace(test$MSSubClass, test$MSSubClass == '150', '20')
test$MSSubClass <- droplevels(test$MSSubClass, exclude =  '150')
```

## Numeric data handling:

Once we have categorical variables in order, we should focus our attention on quantitative data. 

```{r,echo=FALSE}
#We extract a dataset only with quantitative data (32 variables)
num_train_dataset <- train %>% select_if(is.numeric)
num_test_dataset <- test %>% select_if(is.numeric)
```

```{r, echo=FALSE}
summary(num_train_dataset)
summary(num_test_dataset)
```

# Exploratory Analysis

## Correlation

To see which the quantitative features that strongly affect home price, checking the correlation may be an interesting idea.

```{r, fig.height=7,fig.width=7,echo=FALSE}
correlation_matrix <- cor(num_train_dataset,use='pairwise.complete.obs')
rownames(correlation_matrix) <- colnames(correlation_matrix) <- 1:ncol(correlation_matrix)
corrplot(correlation_matrix, method = "color", tl.col = "black", title="Correlation Matrix",tl.srt = 0,tl.cex = 0.7, type="upper")
```

```{r, cache=FALSE, message=FALSE,warning=FALSE,echo=FALSE}
ggcorr(num_train_dataset, label = TRUE, label_size = 2.9, hjust = 1, digits =3,layout.exp = 2)
```

The response variable, Sale Price, is strongly influenciated by the GrLivArea, GarageArea and GarageCars. Other important variables are FullBath, YearBuild, YearRemodAdd or TotalBsmtSF.

```{r, echo=FALSE}
ggplot(train, aes(x = factor(GarageCars), y = SalePrice, fill = factor(GarageCars))) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  labs(x = "GarageCars", y = "SalePrice", title = "Bar Plot: GarageCars & SalePrice") +
  scale_fill_discrete(name = "GarageCars")
```

We can see that the greater the size of the garage (in car capacity), the higher the sale price of the house tend to be. 

```{r,echo=FALSE}
ggplot(train, aes(x = YearBuilt, y = SalePrice)) +
  geom_line() +
  labs(x = "YearBuilt", y = "SalePrice", title = "Line Plot: SalePrice Vs YearBuilt")
```

We see that the trend is positive, the newer the house is, the greater the house would be. Therefore, it makes sense that they have a positive correlation.

```{r,echo=FALSE}
ggplot(train, aes(x = GrLivArea, y = SalePrice)) +
  geom_line() +
  labs(x = "GrLivArea", y = "SalePrice", title = "Line Plot: SalePrice Vs GrLivArea")
```

The last graph reinforce the idea of positive and high correlation between GrLivArea and SalePrice. Naturally, bigger house are more expensive.

When it comes to the correlation between numerical predictions we see that there is high correlation between:

- GarageCars - GarageArea

- TotalBsmtSF - X1stFlr

- TotRmsAbvGrd - GrLivArea

Nonetheless, none of them have perfect correlation, so we cannot delete any variable.

Finally, we wanted to mentioned the seasonality appreciated in the variable YearSold, which surprised us. It can be caused by the economic cycles, in contraction times, despite being the houses cheaper, buyer do not tend to have much savings.

```{r, echo=FALSE}
##ARREGLAR ESTE CODIGO:
#ggplot(train, aes(x = YearSold)) +geom_histogram(binwidth = 1, fill = "skyblue", color = "black", alpha = 0.7) +labs(x = "Año de Venta", y = "Frecuencia", title = "Histograma de Año de Venta")

#ggplot(train, aes(x = YearSold)) +geom_density(fill = "skyblue", color = "black", alpha = 0.7) +labs(x = "Año de Venta", y = "Densidad", title = "Gráfica de Densidad de Año de Venta")
```


```{r,include=FALSE}
#In the first part, we will carry out a descriptive analysis of several variables to understand them better. Graphs will help us to reach this goal, histograms for continuous variables and frequency tables for categorical variables.

#Box-Plots
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen", "lightblue")

#1 is sale price
for (i in 2:13) {
  col_name <- colnames(num_train_dataset)[i]
  boxplot(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 1], 
       breaks = 5)
}
```

```{r,include=FALSE}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen","lightblue")

for (i in 14:25) {
  col_name <- colnames(num_train_dataset)[i]
  boxplot(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 13],
       breaks = 5)
}
```

```{r,include=FALSE}
par(mfrow = c(3, 3))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan")

for (i in 26:31) {
  col_name <- colnames(num_train_dataset)[i]
  boxplot(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 25],
       breaks = 5)
}
```

```{r,include=FALSE}
#Histograms
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen", "lightblue")

for (i in 1:12) {
  col_name <- colnames(num_train_dataset)[i]
  hist(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 1], 
       breaks = 5)
}
```

```{r,include=FALSE}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta", "darkgreen","lightblue")

for (i in 13:24) {
  col_name <- colnames(num_train_dataset)[i]
  hist(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 12],
       breaks = 5)
}
```

```{r, include=FALSE}
par(mfrow = c(3, 4))
colours <- c("red", "blue", "green",
             "orange", "purple", "pink",
             "brown", "gray", "cyan", "magenta")

for (i in 25:31) {
  col_name <- colnames(num_train_dataset)[i]
  hist(num_train_dataset[, i], 
       main = col_name,
       xlab = "Values", 
       col = colours[i - 24],
       breaks = 5)
}
```

```{r,include=FALSE}
#It would be useful to analyse the categorical variables, seeing most popular categories for each variables.
# Lista de columnas categóricas
columnas_categoricas <- colnames(train)[sapply(train, is.factor)]
columnas_categoricas
tablas_frecuencia <- list()
#INTENTO 1
# Iterar sobre cada columna categórica y crear la tabla de frecuencia
for (col in columnas_categoricas) {
  tablas_frecuencia[[col]] <- table(train[[col]])
}
# Mostrar las tablas de frecuencia
tablas_frecuencia
```

## Variable Transformations

```{r, warning=FALSE, message=FALSE, include=FALSE}
#Next step is to get the scatter and density plots for the quantitative features (31 variables) in order to decide which transformation we will apply them (just if it is necessary).
ggpairs(train[,1:12],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
ggpairs(train[,c(1,13:24)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
ggpairs(train[,c(1,25:31)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r warning=FALSE, message=FALSE,include=FALSE}
#This code is used to obtain the skewnees, which help us to decide also the variables to transform
skewness <- round(skew(train[,1:31]), 1)
result_df <- data.frame(variable = colnames(train[,1:31]), skewness = skewness)
pander(result_df)
```

```{r,include=FALSE}
# Check if the quantitative feature has a cero or not. This will influence our choice for the tranformation
counter <- 0
for (i in head(names(train), 31)) {
  if (any(!is.na(train[[i]]) & train[[i]] == 0)) {
    counter <- counter + 1
    print(paste("Column '", i, "' has at least one zero value.", sep = ""))
  }
}
```

```{r apply_transformations,include=FALSE}
# sqrt negative
train[,c(14)] <- sqrt(max(train[,c(14)])+1-train[,c(14)])
test[,c(13)] <- sqrt(max(test[,c(13)])+1-test[,c(13)])
# sqrt positive
train[,c(9)] <- sqrt(train[,c(9)])
test[,c(8)] <- sqrt(test[,c(8)])
# log
train[,c(2,11,13,14)] <- log(train[,c(2,11,13,14)])
test[,c(1,10,12,13)] <- log(test[,c(1,10,12,13)])
# log+1 because there are 0 values on that feature
train[,c(3,6,7,10,12,16,17,22,24,25,27,29)] <- log(train[,c(3,6,7,10,12,16,17,22,24,25,27,29)]+1)
test[,c(2,5,6,9,11,15,16,21,23,24,26,28)] <- log(test[,c(2,5,6,9,11,15,16,21,23,24,26,28)]+1)
```

Different transformation have been applied to different variables, they are:

  - 
  
  - Square root: 
  
  - Logarithm: Applied to the variables that have a positive skewness, that are right-tailed. 
  
  - Logarithm (X+1): Same argue as above, but this was done for variables that have zero values, so as to avoid errors. It was applied to 


```{r, warning=FALSE, message=FALSE, include=FALSE}
#In order to check if we archive the normality distributions on the variables thanks to the transformations apply, we plot again the scatters plots.
ggpairs(train[,1:12],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE,include=FALSE}
ggpairs(train[,c(1,13:24)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

```{r, warning=FALSE, message=FALSE,include=FALSE}
ggpairs(train[,c(1,25:31)],aes(fill="pink"),
        lower = list(continuous = "points"),
        title = "Scatter Plot Matrix") +
  theme_light(base_size=8)
```

Exploratory analysis has helped us to realize that other variables have lot of zero values, which make impossible to normalize them. Arguing the same idea explained in the data-preprocessing, we will delete from our database. The variables that will be excluded are:

- BsmtFinSF2

- EnclosedPorch

- X3SsnPorch

- ScreenPorch

- BsmtHalfBath

```{r, include=FALSE}
train<-train[,-c(8,18,19,20,23)]
test<-test[,-c(7,17,18,19,22)]
```



# Model:

Cambiamos el orden de los factores, a que el base sea "None".

```{r}
train$GarageCond <- factor(train$GarageCond, levels=levels(train$GarageCond)[c(4,5,2,6,3,1)])
test$GarageCond <- factor(test$GarageCond, levels=levels(test$GarageCond)[c(4,5,2,6,3,1)])
```


Separamos en TRAIN_TRAIN (70%) Y TRAIN_VALIDATION (30%)

```{r}
set.seed(33)
samplesize <- round(0.7 * nrow(train), 0)
index <- sample(seq_len(nrow(train)), size = samplesize)

train_train <- train[index, ]
train_validation  <- train[-index, ]
```

## Miramos las categorias disferentes entre train_train y train_validation

```{r}
categorias_diferentes2 <- obtener_diferencia_categorias(train_train[,27:63], train_validation[,27:63])
categorias_diferentes3 <- obtener_diferencia_categorias(train_validation[,27:63],train_train[,27:63])
# Muestra los nombres de las categorías únicas presentes en dataframe_1 pero no en dataframe_2
print("Nombres de categorías presentes en dataframe_1 pero no en dataframe_2:")
print(categorias_diferentes2)
#print(categorias_diferentes3)
```

```{r}
# Para categorias_diferentes3
train_validation$Functional <- replace(train_validation$Functional, train_validation$Functional == "Sev", "Typ")
train_train$Functional <- droplevels(train_train$Functional, exclude= "Sev")
train_validation$Functional <- droplevels(train_validation$Functional, exclude= "Sev")
train$Functional <- replace(train$Functional, train$Functional == "Sev", "Typ")
train$Functional <- droplevels(train$Functional, exclude= "Sev")
test$Functional <- replace(test$Functional, test$Functional == "Sev", "Typ")
test$Functional <- droplevels(test$Functional, exclude= "Sev")


train_validation$ExterCond <- replace(train_validation$ExterCond, train_validation$ExterCond == "Po", "TA")
train_train$ExterCond <- droplevels(train_train$ExterCond, exclude= "Po")
train_validation$ExterCond <- droplevels(train_validation$ExterCond, exclude= "Po")
train$ExterCond <- replace(train$ExterCond, train$ExterCond == "Po", "TA")
train$ExterCond <- droplevels(train$ExterCond, exclude= "Po")
test$ExterCond <- replace(test$ExterCond, test$ExterCond == "Po", "TA")
test$ExterCond <- droplevels(test$ExterCond, exclude= "Po")


train_validation$Exterior2nd <- replace(train_validation$Exterior2nd, train_validation$Exterior2nd == "CBlock", "VinylSd")
train_train$Exterior2nd <- droplevels(train_train$Exterior2nd, exclude= "CBlock")
train_validation$Exterior2nd <- droplevels(train_validation$Exterior2nd, exclude= "CBlock")
train$Exterior2nd <- replace(train$Exterior2nd, train$Exterior2nd == "CBlock", "VinylSd")
train$Exterior2nd <- droplevels(train$Exterior2nd, exclude= "CBlock")
test$Exterior2nd <- replace(test$Exterior2nd, test$Exterior2nd == "CBlock", "VinylSd")
test$Exterior2nd <- droplevels(test$Exterior2nd, exclude= "CBlock")


train_validation$Exterior1st <- replace(train_validation$Exterior1st, train_validation$Exterior1st == "CBlock", "VinylSd")
train_train$Exterior1st <- droplevels(train_train$Exterior1st, exclude= "CBlock")
train_validation$Exterior1st <- droplevels(train_validation$Exterior1st, exclude= "CBlock")
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "CBlock", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude= "CBlock")
test$Exterior1st <- replace(test$Exterior1st, test$Exterior1st == "CBlock", "VinylSd")
test$Exterior1st <- droplevels(test$Exterior1st, exclude= "CBlock")


# Esto es para categorias_diferentes2
train_train$Neighborhood <- replace(train_train$Neighborhood, train_train$Neighborhood == "Blueste", "NAmes")
train_validation$Neighborhood <- droplevels(train_validation$Neighborhood, exclude= "Blueste")
train_train$Neighborhood <- droplevels(train_train$Neighborhood, exclude= "Blueste")
train$Neighborhood <- replace(train$Neighborhood, train$Neighborhood == "Blueste", "NAmes")
train$Neighborhood <- droplevels(train$Neighborhood, exclude= "Blueste")
test$Neighborhood <- replace(test$Neighborhood, test$Neighborhood == "Blueste", "NAmes")
test$Neighborhood <- droplevels(test$Neighborhood, exclude= "Blueste")


train_train$OverallQual <- replace(train_train$OverallQual, train_train$OverallQual == "1", "5")
train_validation$OverallQual <- droplevels(train_validation$OverallQual, exclude= "1")
train_train$OverallQual <- droplevels(train_train$OverallQual, exclude= "1")
train$OverallQual <- replace(train$OverallQual, train$OverallQual == "1", "5")
train$OverallQual <- droplevels(train$OverallQual, exclude= "1")
test$OverallQual <- replace(test$OverallQual, test$OverallQual == "1", "5")
test$OverallQual <- droplevels(test$OverallQual, exclude= "1")


train_train$OverallCond <- replace(train_train$OverallCond, train_train$OverallCond == "1", "5")
train_validation$OverallCond <- droplevels(train_validation$OverallCond, exclude= "1")
train_train$OverallCond <- droplevels(train_train$OverallCond, exclude= "1")
train$OverallCond <- replace(train$OverallCond, train$OverallCond == "1", "5")
train$OverallCond <- droplevels(train$OverallCond, exclude= "1")
test$OverallCond <- replace(test$OverallCond, test$OverallCond == "1", "5")
test$OverallCond <- droplevels(test$OverallCond, exclude= "1")


train_train$Exterior1st <- replace(train_train$Exterior1st, train_train$Exterior1st == "AsphShn", "VinylSd")
train_train$Exterior1st <- droplevels(train_train$Exterior1st, exclude= "AsphShn")
train_validation$Exterior1st <- droplevels(train_validation$Exterior1st, exclude= "AsphShn")
train$Exterior1st <- replace(train$Exterior1st, train$Exterior1st == "AsphShn", "VinylSd")
train$Exterior1st <- droplevels(train$Exterior1st, exclude= "AsphShn")
test$Exterior1st <- replace(test$Exterior1st, test$Exterior1st == "AsphShn", "VinylSd")
test$Exterior1st <- droplevels(test$Exterior1st, exclude= "AsphShn")


train_train$ExterCond <- replace(train_train$ExterCond, train_train$ExterCond == "Ex", "TA")
train_train$ExterCond <- droplevels(train_train$ExterCond, exclude= "Ex")
train_validation$ExterCond <- droplevels(train_validation$ExterCond, exclude= "Ex")
train$ExterCond <- replace(train$ExterCond, train$ExterCond == "Ex", "TA")
train$ExterCond <- droplevels(train$ExterCond, exclude= "Ex")
test$ExterCond <- replace(test$ExterCond, test$ExterCond == "Ex", "TA")
test$ExterCond <- droplevels(test$ExterCond, exclude= "Ex")


train_train$Foundation <- replace(train_train$Foundation, train_train$Foundation == "Wood", "PConc")
train_train$Foundation <- droplevels(train_train$Foundation, exclude= "Wood")
train_validation$Foundation <- droplevels(train_validation$Foundation, exclude= "Wood")
train$Foundation <- replace(train$Foundation, train$Foundation == "Wood", "PConc")
train$Foundation <- droplevels(train$Foundation, exclude= "Wood")
test$Foundation <- replace(test$Foundation, test$Foundation == "Wood", "PConc")
test$Foundation <- droplevels(test$Foundation, exclude= "Wood")


train_train$BsmtCond <- replace(train_train$BsmtCond, train_train$BsmtCond == "Po", "TA")
train_train$BsmtCond <- droplevels(train_train$BsmtCond, exclude= "Po")
train_validation$BsmtCond <- droplevels(train_validation$BsmtCond, exclude= "Po")
train$BsmtCond <- replace(train$BsmtCond, train$BsmtCond == "Po", "TA")
train$BsmtCond <- droplevels(train$BsmtCond, exclude= "Po")
test$BsmtCond <- replace(test$BsmtCond, test$BsmtCond == "Po", "TA")
test$BsmtCond <- droplevels(test$BsmtCond, exclude= "Po")


train_train$HeatingQC <- replace(train_train$HeatingQC, train_train$HeatingQC == "Po", "Ex")
train_train$HeatingQC <- droplevels(train_train$HeatingQC, exclude= "Po")
train_validation$HeatingQC <- droplevels(train_validation$HeatingQC, exclude= "Po")
train$HeatingQC <- replace(train$HeatingQC, train$HeatingQC == "Po", "Ex")
train$HeatingQC <- droplevels(train$HeatingQC, exclude= "Po")
test$HeatingQC <- replace(test$HeatingQC, test$HeatingQC == "Po", "Ex")
test$HeatingQC <- droplevels(test$HeatingQC, exclude= "Po")


train_train$SaleType <- replace(train_train$SaleType, train_train$SaleType == "Con", "WD")
train_train$SaleType <- droplevels(train_train$SaleType, exclude= "Con")
train_validation$SaleType <- droplevels(train_validation$SaleType, exclude= "Con")
train$SaleType <- replace(train$SaleType, train$SaleType == "Con", "WD")
train$SaleType <- droplevels(train$SaleType, exclude= "Con")
test$SaleType <- replace(test$SaleType, test$SaleType == "Con", "WD")
test$SaleType <- droplevels(test$SaleType, exclude= "Con")


train_train$SaleType <- replace(train_train$SaleType, train_train$SaleType == "Oth", "WD")
train_train$SaleType <- droplevels(train_train$SaleType, exclude= "Oth")
train_validation$SaleType <- droplevels(train_validation$SaleType, exclude= "Oth")
train$SaleType <- replace(train$SaleType, train$SaleType == "Oth", "WD")
train$SaleType <- droplevels(train$SaleType, exclude= "Oth")
test$SaleType <- replace(test$SaleType, test$SaleType == "Oth", "WD")
test$SaleType <- droplevels(test$SaleType, exclude= "Oth")
```


```{r}
model1 <- lm(SalePrice~.,data=train_train)
```

If we were to try to reduce the number of variables using vif, we realize that it returns an error as some variables have a correlation of 1 between them. However, this does not have to do between numeric variables, as we can calculate the correlation values between them and see that none of them is zero:
{r, cache=FALSE, message=FALSE,warning=FALSE}
ggcorr(num_train_dataset, label = TRUE, label_size = 2.9, hjust = 1, digits =3,layout.exp = 2)


Therefore, the problem is with categorical variables. For getting the values that are causing problems we use the alias function.

````{r}
alias(model1)
```

We check those variables where the correlation between them has either 1 or -1. First we see, correlation for KitchenAbvGr and: MsSubClass and BldgType, so we delete KitchenAbvGr. Computing again the alias we see correlation for MsSubClass and HouseStyle and delete the latter. 
Our criteria for choosing the variable to delete is based on the description and the variable and the intuition on which one affect more to the sale price. Another important criterion is try to delete those variables that has ones with many variables. 
With the alias function we delete GarageCond, BldgType, Exterior2nd, BsmtQual, BsmtExposure, BsmtFinType1 and GarageType.

```{r}
model2 <- lm(SalePrice~.-Exterior2nd -BsmtQual -BsmtCond -GarageCond -BldgType -BsmtFinType1 -GarageFinish,data=train_train)
alias(model2)
```

The following step is to use step AIC.

```{r}
model2aic <- stepAIC(model2,direction="both", trace=FALSE)
summary(model2aic)
```

Esto no tendría que ir aquí
```{r, cache=FALSE, message=FALSE,warning=FALSE}
# RMSE of train dataset
RMSE(model2aic$fitted.values, train_train$SalePrice)
```

(REsults are no staisfactory, we next try to check for multicolinearity)

## Multicoliniarity 

**Checking the VIF**

```{r, cache=FALSE, message=FALSE,warning=FALSE}
vif(model2aic)
```

We delete `TotalBsmtSF` . GVIF^(1/(2*Df))=8.489150 . We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif1<- lm(formula = SalePrice ~ LotArea + YearBuilt + YearRemodAdd + 
    BsmtFinSF1 + BsmtUnfSF + X2ndFlrSF + GrLivArea + 
    GarageYrBlt + GarageArea + WoodDeckSF + BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtExposure + Heating + 
    CentralAir + Electrical + KitchenQual + Functional + GarageType + 
    SaleCondition, data = train_train)
vif(modelvif1)
```

We remove `X2ndFlrSF` the GVIF^(1/(2*Df))=4.533315. We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif2<- lm(formula = SalePrice ~ LotArea + YearBuilt + YearRemodAdd + 
    BsmtFinSF1 + BsmtUnfSF + GrLivArea + 
    GarageYrBlt + GarageArea + WoodDeckSF + BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtExposure + Heating + 
    CentralAir + Electrical + KitchenQual + Functional + GarageType + 
    SaleCondition, data = train_train)
vif(modelvif2)
```


We remove `YearBuilt` the GVIF^(1/(2*Df))=4.076025. We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif3 <- lm(formula = SalePrice ~ LotArea + YearRemodAdd + BsmtFinSF1 + 
                 BsmtUnfSF + GrLivArea + GarageYrBlt + GarageArea + WoodDeckSF +
                 BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtExposure + Heating + 
    CentralAir + Electrical + KitchenQual + Functional + GarageType + 
    SaleCondition, data = train_train)
vif(modelvif3)
```

We remove `GarageYrBlt` the GVIF^(1/(2*Df))=3.937849. We have to compare with $\sqrt{10}\approx$ 3.2

```{r}
modelvif4 <- lm(formula = SalePrice ~ LotArea + YearRemodAdd + BsmtFinSF1 + 
                 BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF +
                 BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MoSold + MSSubClass + MSZoning + LotConfig + LandSlope + 
    Neighborhood + Condition1 + HouseStyle + OverallQual + OverallCond + 
    RoofMatl + Exterior1st + Foundation + BsmtExposure + Heating + 
    CentralAir + Electrical + KitchenQual + Functional + GarageType + 
    SaleCondition, data = train_train)

vif(modelvif4)
```

Ya no hay multicolinealidad.

```{r}
summary(modelvif4)
```

Once the multicolinearity is managed, we check the RMSE again.

```{r, cache=FALSE, message=FALSE,warning=FALSE}
# RMSE of train dataset
RMSE(modelvif4$fitted.values, train_train$SalePrice)
```


## Interaction between categorical and numeric variables

The following code, give us the p-value between 2 models. It also creates a file to storage the final results, because this is a process that is a little bit time-consuming. After it will help us to check and compare the p-values for significant iterations.

With the next 2 chunk the idea is to find the most significant interaction between variables so we can incorporate them into the final model.
Usually, we mark as a significant interaction when p<0.05. However, because we have obtained 30 interactions, we decided to lower such value into 0.005. With this setting (p<0.005) we just find 1 relevant interaction.


```{r}
dependent_variable <- "SalePrice"
name_vars <- c('LotArea' , 'YearRemodAdd' , 'BsmtFinSF1' , 
    'BsmtUnfSF' , 'GrLivArea' , 'GarageYrBlt', 'GarageArea' , 'WoodDeckSF' , 
    'BsmtFullBath' , 'FullBath' , 'HalfBath' , 'BedroomAbvGr' , 'KitchenAbvGr', 'Fireplaces' ,
    'GarageCars' , 'MoSold' , 'MSSubClass' , 'MSZoning' , 'LotConfig' , 
    'LandSlope' , 'Neighborhood' , 'Condition1' , 'HouseStyle' , 'OverallQual' , 
    'OverallCond' , 'RoofMatl' , 'Exterior1st' , 'Foundation' , 'BsmtExposure' , 
    'Heating' , 'CentralAir' , 'Electrical', 'KitchenQual' , 'Functional' , 'GarageType',
    'SaleCondition')
interaction_matrix <- matrix(0, ncol =  length(name_vars), nrow =  length(name_vars), dimnames = list(name_vars,name_vars))

# Loop through all pairs of variables
for (i in 1:(length(name_vars))) {
  variable1 <- name_vars[i]
  for (j in (i): length(name_vars)) {
    variable2 <- name_vars[j]
    if (variable1 != dependent_variable && variable2 != dependent_variable && variable1 != variable2) {
      # Run both models
      lmod_all_vars <- lm(as.formula(paste(dependent_variable, "~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtExposure + 
    Heating + CentralAir + Electrical + KitchenQual + Functional + 
    GarageType + SaleCondition +", variable1, ":", variable2)), data = train_train)
      lmod_without_inter <- lm(as.formula(paste(dependent_variable, "~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtExposure + 
    Heating + CentralAir + Electrical + KitchenQual + Functional + 
    GarageType + SaleCondition -", variable1, ":", variable2)), data = train_train)
    
      anova_result <- anova(lmod_all_vars, lmod_without_inter)
      interaction_matrix[variable1, variable2] <- anova_result[2, "Pr(>F)"]
    }
  }
}

# Print the significant interaction matrix
print(interaction_matrix)
# Save the significant interaction matrix to a CSV file
write.csv(interaction_matrix, file = "anova_matrix_05.csv", row.names = TRUE)
```

The next code, allow to get the interactions with test value lower than 0.05:

```{r}
anova_05 <- read.csv("anova_matrix_05.csv", sep=",")
# Extract row and column names where values are lower than 0.05
significant_interactions <- which(anova_05 < 0.001 & anova_05>0, arr.ind = TRUE)

# Print the significant interactions
for (i in 1:nrow(significant_interactions)) {
  row_index <- rownames(anova_05)[significant_interactions[i, 1]]
  col_index <- colnames(anova_05)[significant_interactions[i, 2]]
  cat(anova_05[row_index,1], ":", col_index, "+")
}
```

*Explicar mejor porque nos quedamos con estas dos* 

+YearRemodAdd : GrLivArea +LotArea : Neighborhood  


```{r}
model_interactions <- lm(formula= SalePrice~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    BsmtUnfSF + GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + 
    FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + 
    GarageCars + MoSold + MSSubClass + MSZoning + LotConfig + 
    LandSlope + Neighborhood + Condition1 + HouseStyle + OverallQual + 
    OverallCond + RoofMatl + Exterior1st + Foundation + BsmtExposure + 
    Heating + CentralAir + Electrical + KitchenQual + Functional + 
    GarageType + SaleCondition + YearRemodAdd : GrLivArea +LotArea : Neighborhood , data = train_train)

summary(model_interactions)
```

```{r}
RMSE(model_interactions$fitted.values, train_train$SalePrice)
```


## Compare reduce model with full one using ANOVA

```{r}
anova(modelvif4, model_interactions)
```

Its better the model with the interactions.

## (MULTICOLINEARITY AGAIN)

```{r}
alias(model_interactions)
```

```{r}
model_interactions_stepAIC <- stepAIC(model_interactions,direction="both", trace=FALSE)
summary(model_interactions_stepAIC)
```

```{r}
vif(model_interactions_stepAIC)
```

Hemos eliminado YearRemodAdd:GrLivArea por el vif.

```{r}
model_interactionsvif <- lm(formula = SalePrice ~ LotArea + YearRemodAdd + BsmtFinSF1 + 
    GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MSSubClass + MSZoning + LotConfig + Neighborhood + Condition1 + 
    HouseStyle + OverallQual + OverallCond + RoofMatl + Exterior1st + 
    Foundation + BsmtExposure + Heating + CentralAir + KitchenQual + 
    Functional + SaleCondition + LotArea:Neighborhood,
    data = train_train)
vif(model_interactionsvif)
```

Al hacer el vif tenemos que quitar: LotArea

```{r}
model_interactionsvif2 <- lm(formula = SalePrice ~ YearRemodAdd + BsmtFinSF1 + 
    GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MSSubClass + MSZoning + LotConfig + Neighborhood + Condition1 + 
    HouseStyle + OverallQual + OverallCond + RoofMatl + Exterior1st + 
    Foundation + BsmtExposure + Heating + CentralAir + KitchenQual + 
    Functional + SaleCondition + LotArea:Neighborhood,
    data = train_train)
vif(model_interactionsvif2)
```

Al hacer el vif tenemos que quitar: Neighborhood  (31.514782)

```{r}
model_interactionsvif3 <- lm(formula = SalePrice ~ YearRemodAdd + BsmtFinSF1 + 
    GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MSSubClass + MSZoning + LotConfig + Condition1 + 
    HouseStyle + OverallQual + OverallCond + RoofMatl + Exterior1st + 
    Foundation + BsmtExposure + Heating + CentralAir + KitchenQual + 
    Functional + SaleCondition + LotArea:Neighborhood,
    data = train_train)
vif(model_interactionsvif3)
```

## Linearity

```{r, cache=FALSE, message=FALSE,warning=FALSE}
resact <- data.frame(residual = model_interactionsvif3$residuals, fitted = model_interactionsvif3$fitted.values)
resact %>% ggplot(aes(fitted, residual)) + geom_point() + geom_smooth() + geom_hline(aes(yintercept = 0)) + 
    theme(panel.grid = element_blank(), panel.background = element_blank())
```

## Normality

Shapiro test

H0 = normality

```{r}
shapiro.test(model_interactionsvif3$residuals)
```

So data do not follow a normal distribution


## Outliers


```{r}
par(mfrow=c(2,2))
plot(model_interactionsvif3)
```



```{r}
# Crear el gráfico de cuantiles normales de los residuos
residual_qqnorm <- qqnorm(model_interactionsvif3$residuals)

# Obtener los índices de los valores atípicos
outliers_indices <- which(abs(model_interactionsvif3$residuals) > 2 * sd(model_interactionsvif3$residuals))

# Acceder a los valores atípicos
outliers <- model_interactionsvif3$residuals[outliers_indices]
```

```{r}
model_outliers <- lm(formula = SalePrice ~ YearRemodAdd + BsmtFinSF1 + 
    GrLivArea + GarageArea + WoodDeckSF + BsmtFullBath + FullBath + 
    HalfBath + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageCars + 
    MSSubClass + MSZoning + LotConfig + Condition1 + 
    HouseStyle + OverallQual + OverallCond + RoofMatl + Exterior1st + 
    Foundation + BsmtExposure + Heating + CentralAir + KitchenQual + 
    Functional + SaleCondition + LotArea:Neighborhood, 
    data = train_train[-c(81,260,551),])
summary(model_outliers)
```

```{r}
par(mfrow=c(2,2))
plot(model_outliers)
```


The model R^2 improves and the residual standard error decrease from 0.10 to 0.08

SOmos incapaces de mejorar, probamos a sacar varaibles no significativas con muchas categorias, a ver si mejoramos las assumptions y el rmse.


```{r}
shapiro.test(model_outliers$residuals)
```

As we can not see a signicative improvement in the model, we remain these outliers in the model. We continue with `model_outliers`

## Hetereosdacity

```{r, cache=FALSE, message=FALSE,warning=FALSE}
resact %>% ggplot(aes(fitted, residual)) + geom_point() + theme_light() + geom_hline(aes(yintercept = 0))
```

```{r, cache=FALSE, message=FALSE,warning=FALSE}
bptest(model_outliers)
```

# Model tunning

Since the assumptions are not satisfied we need to explore possible transformations of the response variable:


```{r, cache=FALSE, message=FALSE,warning=FALSE}
model_outliers.boxcox=boxcox(model_outliers,lambda = seq(-10, 2, length.out = 10))
model_outliers.boxcox$x[which.max(model_outliers.boxcox$y)]
```


# Model Performance

```{r, cache=FALSE, message=FALSE,warning=FALSE}
# RMSE of train_train
RMSE(model_outliers$fitted.values, train_train[-c(81,260,551),]$SalePrice)
# RMSE of train_validation
RMSE(predict(model_outliers, newdata=train_validation), train_validation$SalePrice)
```


```{r}
predicciones_train=predict(model_outliers, newdata=train)
RMSE(predicciones_train, train$SalePrice)
predicciones_test=exp(predict(model_outliers, newdata=test))
```

Exportamos a un csv las `predicciones_test`.

```{r}
write.csv(data.frame(Id=test_original$Id, SalePrice = predicciones_test), file = "predicciones_test.csv", row.names = FALSE)
```


